
@article{abadiTensorFlowLargeScaleMachine2016,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  shorttitle = {{{TensorFlow}}},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2016},
  month = mar,
  abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
  archivePrefix = {arXiv},
  eprint = {1603.04467},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/VCSA9V6V/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf;/Users/antonio/Zotero/storage/IXJATFHM/1603.html},
  journal = {arXiv:1603.04467 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{agarapDeepLearningUsing2019,
  title = {Deep {{Learning}} Using {{Rectified Linear Units}} ({{ReLU}})},
  author = {Agarap, Abien Fred},
  year = {2019},
  month = feb,
  abstract = {We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer \$h\_\{n - 1\}\$ in a neural network, then multiply it by weight parameters \$\textbackslash theta\$ to get the raw scores \$o\_\{i\}\$. Afterwards, we threshold the raw scores \$o\_\{i\}\$ by \$0\$, i.e. \$f(o) = \textbackslash max(0, o\_\{i\})\$, where \$f(o)\$ is the ReLU function. We provide class predictions \$\textbackslash hat\{y\}\$ through argmax function, i.e. argmax \$f(x)\$.},
  archivePrefix = {arXiv},
  eprint = {1803.08375},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/277BWY94/Agarap - 2019 - Deep Learning using Rectified Linear Units (ReLU).pdf;/Users/antonio/Zotero/storage/GWMN6LZE/1803.html},
  journal = {arXiv:1803.08375 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@techreport{agliettaResponsePierreAuger2005,
  title = {Response of the {{Pierre Auger Observatory}} Water {{Cherenkov}} Detectors to Muons},
  author = {Aglietta, M. and Allison, P. and Andres, E. C. and Arneodo, F. and Bertou, Xavier and Bonifazi, C. and Busca, N. and Creusot, A. and Deligny, O. and Dornic, D. and Genolini, B. and Ghia, P. L. and Grunfeld, C. M. and {Lhenry-Yvon}, I. and Mazur, P. O. and Moreno, E. and Perez, G. and Salazar, H. and Suomijarvi, T.},
  year = {2005},
  month = jul,
  institution = {{Fermi National Accelerator Lab. (FNAL), Batavia, IL (United States)}},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  file = {/Users/antonio/Zotero/storage/RYVPZCLF/Aglietta et al. - 2005 - Response of the Pierre Auger Observatory water Che.pdf;/Users/antonio/Zotero/storage/3WXG8P5B/15020240.html},
  language = {English},
  number = {FERMILAB-CONF-05-282-E-TD}
}

@misc{AguillenATCMlexperiment,
  title = {{{aguillenATC}}/Ml-Experiment},
  abstract = {ml-experiment is a framework for machine learning. It has many functionalities including scalability through docker instances and reproducibility. - aguillenATC/ml-experiment},
  file = {/Users/antonio/Zotero/storage/TFP45YR3/ml-experiment.html},
  howpublished = {https://github.com/aguillenATC/ml-experiment},
  journal = {GitHub},
  language = {en}
}

@misc{AIPlatform,
  title = {{{AI Platform}}},
  abstract = {A development platform to build AI applications that run on GCP and on-premises. Take your ML projects to production, quickly and cost-effectively.},
  file = {/Users/antonio/Zotero/storage/JVHRASP7/ai-platform.html},
  howpublished = {https://cloud.google.com/ai-platform},
  journal = {Google Cloud},
  language = {en}
}

@inproceedings{akibaOptunaNextgenerationHyperparameter2019,
  title = {Optuna: {{A Next}}-Generation {{Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  pages = {2623--2631},
  publisher = {{Association for Computing Machinery}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1145/3292500.3330701},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  file = {/Users/antonio/Zotero/storage/XFP7FA7A/Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimizat.pdf},
  isbn = {978-1-4503-6201-6},
  keywords = {Bayesian optimization,black-box optimization,hyperparameter optimization,machine learning system},
  series = {{{KDD}} '19}
}

@article{albertScienceCaseWide2019,
  title = {Science {{Case}} for a {{Wide Field}}-of-{{View Very}}-{{High}}-{{Energy Gamma}}-{{Ray Observatory}} in the {{Southern Hemisphere}}},
  author = {Albert, A. and Alfaro, R. and Ashkar, H. and Alvarez, C. and {\'A}lvarez, J. and {Arteaga-Vel{\'a}zquez}, J. C. and Solares, H. A. Ayala and Arceo, R. and Bellido, J. A. and BenZvi, S. and Bretz, T. and Brisbois, C. A. and Brown, A. M. and Brun, F. and {Caballero-Mora}, K. S. and Carosi, A. and Carrami{\~n}ana, A. and Casanova, S. and Chadwick, P. M. and Cotter, G. and De Le{\'o}n, S. Couti{\~n}o and Cristofari, P. and Dasso, S. and {de la Fuente}, E. and Dingus, B. L. and Desiati, P. and Salles, F. de O. and {de Souza}, V. and Dorner, D. and {D{\'i}az-V{\'e}lez}, J. C. and {Garc{\'i}a-Gonz{\'a}lez}, J. A. and DuVernois, M. A. and Di Sciascio, G. and Engel, K. and Fleischhack, H. and Fraija, N. and Funk, S. and Glicenstein, J.-F. and Gonzalez, J. and Gonz{\'a}lez, M. M. and Goodman, J. A. and Harding, J. P. and Haungs, A. and Hinton, J. and Hona, B. and Hoyos, D. and Huentemeyer, P. and Iriarte, A. and {Jardin-Blicq}, A. and Joshi, V. and Kaufmann, S. and Kawata, K. and Kunwar, S. and Lefaucheur, J. and Lenain, J.-P. and Link, K. and {L{\'o}pez-Coto}, R. and Marandon, V. and Mariotti, M. and {Mart{\'i}nez-Castro}, J. and {Mart{\'i}nez-Huerta}, H. and Mostaf{\'a}, M. and Nayerhoda, A. and Nellen, L. and Wilhelmi, E. de O{\~n}a and Parsons, R. D. and Patricelli, B. and Pichel, A. and Piel, Q. and Prandini, E. and Pueschel, E. and Procureur, S. and Reisenegger, A. and Rivi{\`e}re, C. and Rodriguez, J. and Rovero, A. C. and Rowell, G. and {Ruiz-Velasco}, E. L. and Sandoval, A. and Santander, M. and Sako, T. and Sako, T. K. and Satalecka, K. and Schoorlemmer, H. and Sch{\"u}ssler, F. and {Seglar-Arroyo}, M. and Smith, A. J. and Spencer, S. and Surajbali, P. and Tabachnick, E. and Taylor, A. M. and Tibolla, O. and Torres, I. and Vallage, B. and Viana, A. and Watson, J. J. and Weisgarber, T. and Werner, F. and White, R. and Wischnewski, R. and Yang, R. and Zepeda, A. and Zhou, H.},
  year = {2019},
  month = feb,
  abstract = {We outline the science motivation for SGSO, the Southern Gamma-Ray Survey Observatory. SGSO will be a next-generation wide field-of-view gamma-ray survey instrument, sensitive to gamma-rays in the energy range from 100 GeV to hundreds of TeV. Its science topics include unveiling galactic and extragalactic particle accelerators, monitoring the transient sky at very high energies, probing particle physics beyond the Standard Model, and the characterization of the cosmic ray flux. SGSO will consist of an air shower detector array, located in South America. Due to its location and large field of view, SGSO will be complementary to other current and planned gamma-ray observatories such as HAWC, LHAASO, and CTA.},
  archivePrefix = {arXiv},
  eprint = {1902.08429},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/M63Q6XPB/Albert et al. - 2019 - Science Case for a Wide Field-of-View Very-High-En.pdf;/Users/antonio/Zotero/storage/CL3U9U7W/1902.html},
  journal = {arXiv:1902.08429 [astro-ph]},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics},
  primaryClass = {astro-ph}
}

@article{albertScienceCaseWide2019a,
  title = {Science {{Case}} for a {{Wide Field}}-of-{{View Very}}-{{High}}-{{Energy Gamma}}-{{Ray Observatory}} in the {{Southern Hemisphere}}},
  author = {Albert, A. and Alfaro, R. and Ashkar, H. and Alvarez, C. and {\'A}lvarez, J. and {Arteaga-Vel{\'a}zquez}, J. C. and Solares, H. A. Ayala and Arceo, R. and Bellido, J. A. and BenZvi, S. and Bretz, T. and Brisbois, C. A. and Brown, A. M. and Brun, F. and {Caballero-Mora}, K. S. and Carosi, A. and Carrami{\~n}ana, A. and Casanova, S. and Chadwick, P. M. and Cotter, G. and De Le{\'o}n, S. Couti{\~n}o and Cristofari, P. and Dasso, S. and {de la Fuente}, E. and Dingus, B. L. and Desiati, P. and Salles, F. de O. and {de Souza}, V. and Dorner, D. and {D{\'i}az-V{\'e}lez}, J. C. and {Garc{\'i}a-Gonz{\'a}lez}, J. A. and DuVernois, M. A. and Di Sciascio, G. and Engel, K. and Fleischhack, H. and Fraija, N. and Funk, S. and Glicenstein, J.-F. and Gonzalez, J. and Gonz{\'a}lez, M. M. and Goodman, J. A. and Harding, J. P. and Haungs, A. and Hinton, J. and Hona, B. and Hoyos, D. and Huentemeyer, P. and Iriarte, A. and {Jardin-Blicq}, A. and Joshi, V. and Kaufmann, S. and Kawata, K. and Kunwar, S. and Lefaucheur, J. and Lenain, J.-P. and Link, K. and {L{\'o}pez-Coto}, R. and Marandon, V. and Mariotti, M. and {Mart{\'i}nez-Castro}, J. and {Mart{\'i}nez-Huerta}, H. and Mostaf{\'a}, M. and Nayerhoda, A. and Nellen, L. and Wilhelmi, E. de O{\~n}a and Parsons, R. D. and Patricelli, B. and Pichel, A. and Piel, Q. and Prandini, E. and Pueschel, E. and Procureur, S. and Reisenegger, A. and Rivi{\`e}re, C. and Rodriguez, J. and Rovero, A. C. and Rowell, G. and {Ruiz-Velasco}, E. L. and Sandoval, A. and Santander, M. and Sako, T. and Sako, T. K. and Satalecka, K. and Schoorlemmer, H. and Sch{\"u}ssler, F. and {Seglar-Arroyo}, M. and Smith, A. J. and Spencer, S. and Surajbali, P. and Tabachnick, E. and Taylor, A. M. and Tibolla, O. and Torres, I. and Vallage, B. and Viana, A. and Watson, J. J. and Weisgarber, T. and Werner, F. and White, R. and Wischnewski, R. and Yang, R. and Zepeda, A. and Zhou, H.},
  year = {2019},
  month = feb,
  abstract = {We outline the science motivation for SGSO, the Southern Gamma-Ray Survey Observatory. SGSO will be a next-generation wide field-of-view gamma-ray survey instrument, sensitive to gamma-rays in the energy range from 100 GeV to hundreds of TeV. Its science topics include unveiling galactic and extragalactic particle accelerators, monitoring the transient sky at very high energies, probing particle physics beyond the Standard Model, and the characterization of the cosmic ray flux. SGSO will consist of an air shower detector array, located in South America. Due to its location and large field of view, SGSO will be complementary to other current and planned gamma-ray observatories such as HAWC, LHAASO, and CTA.},
  archivePrefix = {arXiv},
  eprint = {1902.08429},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/8PXLV83F/Albert et al. - 2019 - Science Case for a Wide Field-of-View Very-High-En.pdf;/Users/antonio/Zotero/storage/Y24PAIRM/1902.html},
  journal = {arXiv:1902.08429 [astro-ph]},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics},
  primaryClass = {astro-ph}
}

@article{allenArtificialIntelligenceRight2020,
  title = {Artificial {{Intelligence}}: The Right to Protection from Discrimination Caused by Algorithms, Machine Learning and Automated Decision-Making},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Allen, Robin and Masters, Dee},
  year = {2020},
  month = mar,
  volume = {20},
  pages = {585--598},
  issn = {1863-9038},
  doi = {10.1007/s12027-019-00582-w},
  abstract = {An analysis of how Artificial Intelligence, ML algorithms and automated decision-making can give rise to discrimination and the ways in which Europe's existing equality framework can regulate any inequality whilst also identifying how it must change to meet the challenges ahead. The authors also examine some of the ways in which the GDPR impacts on Artificial Intelligence, ML algorithms and automated decision making.},
  file = {/Users/antonio/Zotero/storage/9CQLFRZ9/Allen and Masters - 2020 - Artificial Intelligence the right to protection f.pdf},
  journal = {ERA Forum},
  language = {en},
  number = {4}
}

@misc{AmazonSageMaker,
  title = {{Amazon SageMaker}},
  abstract = {Amazon SageMaker es un servicio completamente administrado que brinda a todos los cient\'ificos de datos y desarrolladores la capacidad de crear, entrenar e implementar modelos de aprendizaje autom\'atico de forma r\'apida. SageMaker elimina las tareas arduas de cada paso del proceso de aprendizaje autom\'atico para que sea m\'as f\'acil crear modelos de alta calidad.},
  file = {/Users/antonio/Zotero/storage/JHM2Q5JR/sagemaker.html},
  howpublished = {https://aws.amazon.com/es/sagemaker/},
  journal = {Amazon Web Services, Inc.},
  language = {es-ES}
}

@book{anderson2010kanban,
  title = {Kanban: Successful Evolutionary Change for Your Technology Business},
  author = {Anderson, David J},
  year = {2010},
  publisher = {{Blue Hole Press}}
}

@article{argiroOfflineSoftwareFramework2007,
  title = {The {{Offline Software Framework}} of the {{Pierre Auger Observatory}}},
  author = {Argiro, S. and Barroso, S. L. C. and Gonzalez, J. and Nellen, L. and Paul, T. and Porter, T. A. and Prado Jr., L. and Roth, M. and Ulrich, R. and Veberic, D.},
  year = {2007},
  month = oct,
  volume = {580},
  pages = {1485--1496},
  issn = {01689002},
  doi = {10.1016/j.nima.2007.07.010},
  abstract = {The Pierre Auger Observatory is designed to unveil the nature and the origins of the highest energy cosmic rays. The large and geographically dispersed collaboration of physicists and the wide-ranging collection of simulation and reconstruction tasks pose some special challenges for the offline analysis software. We have designed and implemented a general purpose framework which allows collaborators to contribute algorithms and sequencing instructions to build up the variety of applications they require. The framework includes machinery to manage these user codes, to organize the abundance of user-contributed configuration files, to facilitate multi-format file handling, and to provide access to event and time-dependent detector information which can reside in various data sources. A number of utilities are also provided, including a novel geometry package which allows manipulation of abstract geometrical objects independent of coordinate system choice. The framework is implemented in C++, and takes advantage of object oriented design and common open source tools, while keeping the user side simple enough for C++ novices to learn in a reasonable time. The distribution system incorporates unit and acceptance testing in order to support rapid development of both the core framework and contributed user code.},
  archivePrefix = {arXiv},
  eprint = {0707.1652},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/LSHIRJCY/Argiro et al. - 2007 - The Offline Software Framework of the Pierre Auger.pdf;/Users/antonio/Zotero/storage/X3BM43A2/0707.html},
  journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  keywords = {Astrophysics},
  number = {3}
}

@article{arnoldAutomatingAIOperations2020,
  title = {Towards {{Automating}} the {{AI Operations Lifecycle}}},
  author = {Arnold, Matthew and Boston, Jeffrey and Desmond, Michael and Duesterwald, Evelyn and Elder, Benjamin and Murthi, Anupama and Navratil, Jiri and Reimer, Darrell},
  year = {2020},
  month = mar,
  abstract = {Today's AI deployments often require significant human involvement and skill in the operational stages of the model lifecycle, including pre-release testing, monitoring, problem diagnosis and model improvements. We present a set of enabling technologies that can be used to increase the level of automation in AI operations, thus lowering the human effort required. Since a common source of human involvement is the need to assess the performance of deployed models, we focus on technologies for performance prediction and KPI analysis and show how they can be used to improve automation in the key stages of a typical AI operations pipeline.},
  archivePrefix = {arXiv},
  eprint = {2003.12808},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/3PECKMWM/Arnold et al. - 2020 - Towards Automating the AI Operations Lifecycle.pdf;/Users/antonio/Zotero/storage/84FFTSFB/2003.html},
  journal = {arXiv:2003.12808 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering,I.2},
  primaryClass = {cs}
}

@misc{Art12GDPR,
  title = {Art. 12 {{GDPR}} \textendash{} {{Transparent}} Information, Communication and Modalities for the Exercise of the Rights of the Data Subject},
  abstract = {1The controller shall take appropriate measures to provide any information referred to in Articles 13 and 14 and any communication under Articles 15 to 22 and 34 relating to processing to the data subject in a concise, transparent, intelligible and easily accessible form, using clear and plain language, in particular for any information addressed specifically \ldots{} Continue reading Art. 12 GDPR \textendash{} Transparent information, communication and modalities for the exercise of the rights of the data subject},
  file = {/Users/antonio/Zotero/storage/E8XAKBBV/art-12-gdpr.html},
  journal = {General Data Protection Regulation (GDPR)},
  language = {en-US}
}

@misc{ArtGDPRPrinciples,
  title = {Art. 5 {{GDPR}} \textendash{} {{Principles}} Relating to Processing of Personal Data},
  abstract = {Personal data shall be: processed lawfully, fairly and in a transparent manner in relation to the data subject (`lawfulness, fairness and transparency'); collected for specified, explicit and legitimate purposes and not further processed in a manner that is incompatible with those purposes; further processing for archiving purposes in the public interest, scientific or historical research \ldots{} Continue reading Art. 5 GDPR \textendash{} Principles relating to processing of personal data},
  file = {/Users/antonio/Zotero/storage/Y6384L2U/art-5-gdpr.html},
  journal = {General Data Protection Regulation (GDPR)},
  language = {en-US}
}

@misc{ArtificialIntelligenceFaces,
  title = {Artificial Intelligence Faces Reproducibility Crisis | {{Science}}},
  file = {/Users/antonio/Zotero/storage/C9RT22PZ/725.html},
  howpublished = {https://science.sciencemag.org/content/359/6377/725.summary}
}

@article{assisLATTESNovelDetector2018,
  title = {{{LATTES}}: A Novel Detector Concept for a Gamma-Ray Experiment in the {{Southern}} Hemisphere},
  shorttitle = {{{LATTES}}},
  author = {Assis, P. and {de Almeida}, U. Barres and Blanco, A. and Concei{\c c}{\~a}o, R. and Piazzoli, B. D'Ettore and De Angelis, A. and Doro, M. and Fonte, P. and Lopes, L. and Matthiae, G. and Pimenta, M. and Shellard, R. and Tom{\'e}, B.},
  year = {2018},
  month = apr,
  abstract = {The Large Array Telescope for Tracking Energetic Sources (LATTES), is a novel concept for an array of hybrid EAS array detectors, composed of a Resistive Plate Counter array coupled to a Water Cherenkov Detector, planned to cover gamma rays from less than 100 GeV up to 100 TeVs. This experiment, to be installed at high altitude in South America, could cover the existing gap in sensitivity between satellite and ground arrays. The low energy threshold, large duty cycle and wide field of view of LATTES makes it a powerful tool to detect transient phenomena and perform long term observations of variable sources. Moreover, given its characteristics, it would be fully complementary to the planned Cherenkov Telescope Array (CTA) as it would be able to issue alerts. In this talk, a description of its main features and capabilities, as well as results on its expected performance, and sensitivity, will be presented.},
  archivePrefix = {arXiv},
  eprint = {1709.09624},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/5QV3K83L/Assis et al. - 2018 - LATTES a novel detector concept for a gamma-ray e.pdf;/Users/antonio/Zotero/storage/VHSKAQFK/1709.html},
  journal = {arXiv:1709.09624 [astro-ph, physics:hep-ex, physics:physics]},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics,High Energy Physics - Experiment,Physics - Instrumentation and Detectors},
  primaryClass = {astro-ph, physics:hep-ex, physics:physics}
}

@article{assuncaoAutomaticDesignArtificial2019,
  title = {Automatic {{Design}} of {{Artificial Neural Networks}} for {{Gamma}}-{{Ray Detection}}},
  author = {Assun{\c c}{\~a}o, Filipe and Correia, Jo{\~a}o and Concei{\c c}{\~a}o, R{\'u}ben and Pimenta, M{\'a}rio and Tom{\'e}, Bernardo and Louren{\c c}o, Nuno and Machado, Penousal},
  year = {2019},
  volume = {7},
  pages = {110531--110540},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2933947},
  abstract = {The goal of this work is to investigate the possibility of improving current gamma/hadron discrimination based on their shower patterns recorded on the ground. To this end we propose the use of Convolutional Neural Networks (CNNs) for their ability to distinguish patterns based on automatically designed features. In order to promote the creation of CNNs that properly uncover the hidden patterns in the data, and at same time avoid the burden of hand-crafting the topology and learning hyper-parameters we resort to NeuroEvolution; in particular we use Fast-DENSER++, a variant of Deep Evolutionary Network Structured Representation. The results show that the best CNN generated by Fast-DENSER++ improves by a factor of 2 when compared with the results reported by classic statistical approaches. Additionally, we experiment ensembling the 10 best generated CNNs, one from each of the evolutionary runs; the ensemble leads to an improvement by a factor of 2.3. These results show that it is possible to improve the gamma/hadron discrimination based on CNNs that are automatically generated and are trained with instances of the ground impact patterns.},
  archivePrefix = {arXiv},
  eprint = {1905.03532},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/IK9NHL5H/Assunção et al. - 2019 - Automatic Design of Artificial Neural Networks for.pdf;/Users/antonio/Zotero/storage/IQTPENDY/1905.html},
  journal = {IEEE Access},
  keywords = {Computer Science - Neural and Evolutionary Computing}
}

@misc{AutoencoderBetaVAE2018,
  title = {From {{Autoencoder}} to {{Beta}}-{{VAE}}},
  year = {2018},
  month = aug,
  abstract = {Autocoders are a family of neural network models aiming to learn compressed latent variables of high-dimensional data. Starting from the basic autocoder model, this post reviews several variations, including denoising, sparse, and contractive autoencoders, and then Variational Autoencoder (VAE) and its modification beta-VAE.},
  file = {/Users/antonio/Zotero/storage/6P8IBZBF/from-autoencoder-to-beta-vae.html},
  howpublished = {https://lilianweng.github.io/2018/08/12/from-autoencoder-to-beta-vae.html},
  journal = {Lil'Log},
  language = {en}
}

@misc{AutoencodersClassifiers,
  title = {Autoencoders as {{Classifiers}}},
  file = {/Users/antonio/Zotero/storage/HSCYAPB6/autoencoders-as-classifiers.html},
  howpublished = {https://radicalrafi.github.io/posts/autoencoders-as-classifiers/}
}

@misc{AWSCloudComputing,
  title = {{AWS | Cloud Computing - Servicios de inform\'atica en la nube}},
  abstract = {Descubra la infraestructura de la nube que ofrece Amazon Web Services, servicios de alojamiento en la nube, servicios inform\'atica en la nube y servicios en la nube para empresas. Reg\'istrate gratis y paga s\'olo por el uso.},
  file = {/Users/antonio/Zotero/storage/WITSSQAS/es.html},
  howpublished = {https://aws.amazon.com/es/},
  journal = {Amazon Web Services, Inc.},
  language = {es-ES}
}

@misc{AzureMachineLearning,
  title = {{Azure Machine Learning | Microsoft Azure}},
  abstract = {Cree e implemente modelos de aprendizaje autom\'atico de manera simplificada con Azure Machine Learning Service. Mejore la accesibilidad al aprendizaje autom\'atico con caracter\'isticas automatizadas.},
  file = {/Users/antonio/Zotero/storage/2KDN4RRG/machine-learning.html},
  howpublished = {https://azure.microsoft.com/es-es/services/machine-learning/},
  language = {es}
}

@article{baker500ScientistsLift2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  author = {Baker, Monya},
  year = {2016},
  month = may,
  volume = {533},
  pages = {452},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  chapter = {News Feature},
  file = {/Users/antonio/Zotero/storage/WFRCRDG6/Baker - 2016 - 1,500 scientists lift the lid on reproducibility.pdf;/Users/antonio/Zotero/storage/2JY3N6EP/1-500-scientists-lift-the-lid-on-reproducibility-1.html},
  journal = {Nature News},
  language = {en},
  number = {7604}
}

@inproceedings{baldi2012autoencoders,
  title = {Autoencoders, Unsupervised Learning, and Deep Architectures},
  booktitle = {Proceedings of {{ICML}} Workshop on Unsupervised and Transfer Learning},
  author = {Baldi, Pierre},
  year = {2012},
  pages = {37--49}
}

@incollection{bar-hillelPresentStatusAutomatic1960,
  title = {The {{Present Status}} of {{Automatic Translation}} of {{Languages}}**{{This}} Article Was Prepared with the Sponsorship of the {{Informations Systems Branch}}, {{Office}} of {{Naval Research}}, under {{Contract NR}} 049130. {{Reproduction}} as a Whole or in Part for the Purposes of the {{U}}. {{S}}. {{Government}} Is Permitted.},
  booktitle = {Advances in {{Computers}}},
  author = {{Bar-Hillel}, Yehoshua},
  editor = {Alt, Franz L.},
  year = {1960},
  month = jan,
  volume = {1},
  pages = {91--163},
  publisher = {{Elsevier}},
  doi = {10.1016/S0065-2458(08)60607-5},
  abstract = {Machine translation (MT) has become a multimillion dollar affair. Fully automatic, high quality translation is not a reasonable goal, not even for scientific texts. This chapter surveys the situations where translation involved has to be of high quality. A human translator, in order to arrive at his/her high quality output, is obliged to make intelligent use of extra linguistic knowledge that sometimes has to be of considerable breadth and depth. Reasonable goals are either fully automatic, low quality translation or partly automatic, or high quality translation. Full automation of the translation process is incompatible with high quality. The two possible directions where a compromise could be struck are sacrificing quality or reducing the self-sufficiency of the machine output. There are many situations where less than high quality machine output is satisfactory. However, when the aim of MT is lowered to that of high quality translation by a machine-post-editor partnership, the decisive problem becomes to determine the region of optimality in the continuum of possible divisions of labor. The exact position of this region will be a function of the state of linguistic analysis where the languages involved are submitted. With machine-time/efficiency becoming cheaper and human time becoming more expensive, continuous efforts will be made to push this region in the direction of reducing the human element. However, there is no good reason to assume that this region can be pushed to the end of the line, certainly not in the near future.},
  file = {/Users/antonio/Zotero/storage/N2Y9P7BE/S0065245808606075.html},
  language = {en}
}

@misc{BasesNormativaOTRI,
  title = {Bases y Normativa {{OTRI}} - {{El}} Portal Web de La Transferencia Del Conocimiento de La {{Universidad}} de {{Granada}}},
  file = {/Users/antonio/Zotero/storage/MHLH6IYC/normativa-y-bases.html},
  howpublished = {https://otri.ugr.es/contratacion\_personal/cms/menu/normativa-y-bases/}
}

@article{bellman1966dynamic,
  title = {Dynamic Programming},
  author = {Bellman, Richard},
  year = {1966},
  volume = {153},
  pages = {34--37},
  publisher = {{American Association for the Advancement of Science}},
  journal = {Science},
  number = {3731}
}

@book{bengioDeepLearning2017a,
  title = {{Deep Learning}},
  author = {Bengio, Yoshua},
  year = {2017},
  month = jan,
  publisher = {{MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives. "Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceX Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  isbn = {978-0-262-03561-3},
  language = {Ingl\'es}
}

@incollection{bengioRegularizedAutoencoders2017,
  title = {{Regularized Autoencoders}},
  booktitle = {{Deep Learning}},
  author = {Bengio, Yoshua},
  year = {2017},
  month = jan,
  pages = {501},
  publisher = {{MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives. "Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceX Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  isbn = {978-0-262-03561-3},
  language = {Ingl\'es}
}

@incollection{bergstraAlgorithmsHyperParameterOptimization2011,
  title = {Algorithms for {{Hyper}}-{{Parameter Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Bergstra, James S. and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  editor = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {2546--2554},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/antonio/Zotero/storage/RA7WL9ID/Bergstra et al. - 2011 - Algorithms for Hyper-Parameter Optimization.pdf;/Users/antonio/Zotero/storage/SXRKR8E3/4443-algorithms-for-hyper-parameter-optimization.html}
}

@article{boehmSystemDSDeclarativeMachine2020,
  title = {{{SystemDS}}: {{A Declarative Machine Learning System}} for the {{End}}-to-{{End Data Science Lifecycle}}},
  shorttitle = {{{SystemDS}}},
  author = {Boehm, Matthias and Antonov, Iulian and Dokter, Mark and Ginthoer, Robert and Innerebner, Kevin and Klezin, Florijan and Lindst{\"a}dt, Stefanie N. and Phani, Arnab and Rath, Benjamin},
  year = {2020},
  abstract = {Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative languages with R-like syntax for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.},
  file = {/Users/antonio/Zotero/storage/8GHKP3GS/Boehm et al. - 2020 - SystemDS A Declarative Machine Learning System fo.pdf},
  journal = {CIDR}
}

@article{boettigerIntroductionDockerReproducible2015,
  title = {An Introduction to {{Docker}} for Reproducible Research, with Examples from the {{R}} Environment},
  author = {Boettiger, Carl},
  year = {2015},
  month = jan,
  volume = {49},
  pages = {71--79},
  issn = {0163-5980},
  doi = {10.1145/2723872.2723882},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a `DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  archivePrefix = {arXiv},
  eprint = {1410.0846},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/RRKKHVFY/Boettiger - 2015 - An introduction to Docker for reproducible researc.pdf;/Users/antonio/Zotero/storage/WMZMWF82/1410.html},
  journal = {ACM SIGOPS Operating Systems Review},
  keywords = {Computer Science - Software Engineering},
  number = {1}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  month = oct,
  volume = {45},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash 156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  file = {/Users/antonio/Zotero/storage/4HG78XE4/Breiman - 2001 - Random Forests.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {1}
}

@article{brunROOTObjectOriented1997,
  title = {{{ROOT}} \textemdash{} {{An}} Object Oriented Data Analysis Framework},
  author = {Brun, Rene and Rademakers, Fons},
  year = {1997},
  month = apr,
  volume = {389},
  pages = {81--86},
  issn = {0168-9002},
  doi = {10.1016/S0168-9002(97)00048-X},
  abstract = {The ROOT system in an Object Oriented framework for large scale data analysis. ROOT written in C++, contains, among others, an efficient hierarchical OO database, a C++ interpreter, advanced statistical analysis (multi-dimensional histogramming, fitting, minimization, cluster finding algorithms) and visualization tools. The user interacts with ROOT via a graphical user interface, the command line or batch scripts. The command and scripting language is C++ (using the interpreter) and large scripts can be compiled and dynamically linked in. The OO database design has been optimized for parallel access (reading as well as writing) by multiple processes.},
  file = {/Users/antonio/Zotero/storage/5KEJ4HHB/Brun and Rademakers - 1997 - ROOT — An object oriented data analysis framework.pdf;/Users/antonio/Zotero/storage/TBRFMYWG/S016890029700048X.html},
  journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  language = {en},
  number = {1},
  series = {New {{Computing Techniques}} in {{Physics Research V}}}
}

@misc{BuildRightAutoencoder,
  title = {Build the Right {{Autoencoder}} \textemdash{} {{Tune}} and {{Optimize}} Using {{PCA}} Principles. {{Part II}}},
  file = {/Users/antonio/Zotero/storage/436WET47/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6.html},
  howpublished = {https://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6}
}

@article{burrowsBaadeZwickySupernovae2015,
  title = {Baade and {{Zwicky}}: ``{{Super}}-Novae,'' Neutron Stars, and Cosmic Rays},
  shorttitle = {Baade and {{Zwicky}}},
  author = {Burrows, Adam S.},
  year = {2015},
  month = feb,
  volume = {112},
  pages = {1241--1242},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1422666112},
  abstract = {In 1934, two astronomers in two of the most prescient papers in the astronomical literature coined the term ``supernova,'' hypothesized the existence of neutron stars, and knit them together with the origin of cosmic-rays to inaugurate one of the most surprising syntheses in the annals of science. From the vantage point of 80 y, the centrality of supernova explosions in astronomical thought would seem obvious. Supernovae are the source of many of the elements of nature, and their blasts roil the interstellar medium in ways that inaugurate and affect star formation and structurally alter the visible component of galaxies at birth. They are the origin of most cosmic-rays, and these energetic rays have pronounced effects in the galaxy, even providing an appreciable fraction of the human radiation doses at the surface of the Earth and in jet flight. Prodiguously bright supernovae can be seen across the Universe and have been used to great effect to take its measure, and a majority of them give birth to impressively dense neutron stars and black holes. Indeed, the radio and X-ray pulsars of popular discourse, novels, and movies are rapidly spinning neutron stars injected into the galaxy upon the eruption of a supernova (Fig. 1). Fig. 1.  A picture of the inner regions of the famous Crab Nebula captures emergent jets and the ``Napoleon Hat'' structure of surrounding plasma. The radio/optical/X-ray pulsar, a neutron star rotating at {$\sim$}30 Hz, is buried in the center. The Crab was produced in a supernova explosion in A.D. 1054. Image courtesy of ESA/NASA. However, it was only with the two startlingly prescient PNAS papers by Baade and Zwicky (1 \ldots{}  [{$\carriagereturn$}][1]1Email: burrows\{at\}astro.princeton.edu.  [1]: \#xref-corresp-1-1},
  chapter = {Commentary},
  file = {/Users/antonio/Zotero/storage/68XCRXG6/Burrows - 2015 - Baade and Zwicky “Super-novae,” neutron stars, an.pdf;/Users/antonio/Zotero/storage/EZ3KX8MJ/1241.html},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {5},
  pmid = {25650425}
}

@dataset{campillo_sanchez_pablo_2020_4252636,
  title = {Spanish\_used\_car\_market: {{Coches}} de Segunda Mano a La Venta En Espa\~na},
  author = {Campillo S{\'a}nchez, Pablo and Uceda Mart{\'i}nez, Pedro},
  year = {2020},
  month = nov,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.4252636}
}

@article{campillo_sanchez_pablo_2020_4252636,
  title = {Spanish\_used\_car\_market: {{Coches}} de Segunda Mano a La Venta En Espa\~na},
  author = {Campillo S{\'a}nchez, Pablo and Uceda Mart{\'i}nez, Pedro},
  year = {2020},
  month = nov,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.4252636}
}

@article{chalapathyDeepLearningAnomaly2019,
  title = {Deep {{Learning}} for {{Anomaly Detection}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Anomaly Detection}}},
  author = {Chalapathy, Raghavendra and Chawla, Sanjay},
  year = {2019},
  month = jan,
  abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
  archivePrefix = {arXiv},
  eprint = {1901.03407},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/VCIDYELT/Chalapathy and Chawla - 2019 - Deep Learning for Anomaly Detection A Survey.pdf;/Users/antonio/Zotero/storage/VTJA967I/1901.html},
  journal = {arXiv:1901.03407 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chengComparingBayesianNetwork2013,
  title = {Comparing {{Bayesian Network Classifiers}}},
  author = {Cheng, Jie and Greiner, Russell},
  year = {2013},
  month = jan,
  abstract = {In this paper, we empirically evaluate algorithms for learning four types of Bayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BN augmented Naive-Bayes and general BNs, where the latter two are learned using two variants of a conditional-independence (CI) based BN-learning algorithm. Experimental results show the obtained classifiers, learned using the CI based algorithms, are competitive with (or superior to) the best known classifiers, based on both Bayesian networks and other formalisms; and that the computational time for learning and using these classifiers is relatively small. Moreover, these results also suggest a way to learn yet more effective classifiers; we demonstrate empirically that this new algorithm does work as expected. Collectively, these results argue that BN classifiers deserve more attention in machine learning and data mining communities.},
  archivePrefix = {arXiv},
  eprint = {1301.6684},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/5EYH3Y89/Cheng and Greiner - 2013 - Comparing Bayesian Network Classifiers.pdf;/Users/antonio/Zotero/storage/MK5UIS5G/1301.html},
  journal = {arXiv:1301.6684 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chenSequentialVAELSTMAnomaly2019,
  title = {Sequential {{VAE}}-{{LSTM}} for {{Anomaly Detection}} on {{Time Series}}},
  author = {Chen, Run-Qing and Shi, Guang-Hui and Zhao, Wan-Lei and Liang, Chang-Hui},
  year = {2019},
  month = oct,
  abstract = {In order to support stable web-based applications and services, anomalies on the IT performance status have to be detected timely. Moreover, the performance trend across the time series should be predicted. In this paper, we propose SeqVL (Sequential VAE-LSTM), a neural network model based on both VAE (Variational Auto-Encoder) and LSTM (Long Short-Term Memory). This work is the first attempt to integrate unsupervised anomaly detection and trend prediction under one framework. Moreover, this model performs considerably better on detection and prediction than VAE and LSTM work alone. On unsupervised anomaly detection, SeqVL achieves competitive experimental results compared with other state-of-the-art methods on public datasets. On trend prediction, SeqVL outperforms several classic time series prediction models in the experiments of the public dataset.},
  archivePrefix = {arXiv},
  eprint = {1910.03818},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/6ZA74HS2/Chen et al. - 2019 - Sequential VAE-LSTM for Anomaly Detection on Time .pdf;/Users/antonio/Zotero/storage/PQ883MH9/1910.html},
  journal = {arXiv:1910.03818 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = {2016},
  month = aug,
  pages = {785--794},
  publisher = {{Association for Computing Machinery}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939785},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  file = {/Users/antonio/Zotero/storage/U9E2IM4Q/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf},
  isbn = {978-1-4503-4232-2},
  keywords = {large-scale machine learning},
  series = {{{KDD}} '16}
}

@article{cheshirePlayerTrackingAnalysis,
  title = {Player {{Tracking}} and {{Analysis}} of {{Basketball Plays}}},
  author = {Cheshire, Evan and Halasz, Cibele and Perin, Jose Krause},
  pages = {6},
  abstract = {We developed an algorithm that tracks the movements of ten different players from a video of a basketball game. With their position tracked, we then proceed to map the position of these players onto an image of a basketball court. The purpose of tracking player is to provide the maximum amount of information to basketball coaches and organizations, so that they can better design mechanisms of defense and attack. Overall, our model has a high degree of identification and tracking of the players in the court.},
  file = {/Users/antonio/Zotero/storage/AQHF9RTR/Cheshire et al. - Player Tracking and Analysis of Basketball Plays.pdf},
  language = {en}
}

@article{chirigatiCollaborativeApproachComputational2016,
  title = {A {{Collaborative Approach}} to {{Computational Reproducibility}}},
  author = {Chirigati, Fernando and Capone, Rebecca and Shasha, Dennis and Rampin, Remi and Freire, Juliana},
  year = {2016},
  month = jul,
  volume = {59},
  pages = {95--97},
  issn = {03064379},
  doi = {10.1016/j.is.2016.03.002},
  abstract = {Although a standard in natural science, reproducibility has been only episodically applied in experimental computer science. Scientific papers often present a large number of tables, plots and pictures that summarize the obtained results, but then loosely describe the steps taken to derive them. Not only can the methods and the implementation be complex, but also their configuration may require setting many parameters and/or depend on particular system configurations. While many researchers recognize the importance of reproducibility, the challenge of making it happen often outweigh the benefits. Fortunately, a plethora of reproducibility solutions have been recently designed and implemented by the community. In particular, packaging tools (e.g., ReproZip) and virtualization tools (e.g., Docker) are promising solutions towards facilitating reproducibility for both authors and reviewers. To address the incentive problem, we have implemented a new publication model for the Reproducibility Section of Information Systems Journal. In this section, authors submit a reproducibility paper that explains in detail the computational assets from a previous published manuscript in Information Systems.},
  archivePrefix = {arXiv},
  eprint = {1709.01154},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/GZF4LUZY/Chirigati et al. - 2016 - A Collaborative Approach to Computational Reproduc.pdf;/Users/antonio/Zotero/storage/M23KFW54/1709.html},
  journal = {Information Systems},
  keywords = {Computer Science - Digital Libraries}
}

@article{clevertFastAccurateDeep2016,
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  year = {2016},
  month = feb,
  abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
  archivePrefix = {arXiv},
  eprint = {1511.07289},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/K6L8ZJ7P/Clevert et al. - 2016 - Fast and Accurate Deep Network Learning by Exponen.pdf;/Users/antonio/Zotero/storage/MRJ2EPAU/1511.html},
  journal = {arXiv:1511.07289 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{collbergMeasuringReproducibilityComputer,
  title = {Measuring {{Reproducibility}} in {{Computer Systems Research}}},
  author = {Collberg, Christian and Moraila, Gina and Shankaran, Akash and Shi, Zuoming and Warren, Alex M},
  pages = {37},
  abstract = {We describe a study into the willingness of Computer Systems researchers to share their code and data. We find that . . . . We also propose a novel sharing specification scheme that will require researchers to specify the level of reproducibility that reviewers and readers can assume from a paper either submitted for publication, or published.},
  file = {/Users/antonio/Zotero/storage/PDWWEXRD/Collberg et al. - Measuring Reproducibility in Computer Systems Rese.pdf},
  language = {en}
}

@inproceedings{collbergMeasuringReproducibilityComputer2014,
  title = {Measuring {{Reproducibility}} in {{Computer Systems Research}}},
  author = {Collberg, Christian S.},
  year = {2014},
  abstract = {We describe a study into the willingness of Computer Systems researchers to share their code and data. We nd that . . . . We also propose a novel sharing specication scheme that will require researchers to specify the level of reproducibility that reviewers and readers can assume from a paper either submitted for publication, or published.},
  file = {/Users/antonio/Zotero/storage/3HHCES5D/Collberg - 2014 - Measuring Reproducibility in Computer Systems Rese.pdf}
}

@article{collinsDeliveringVisionMLOps,
  title = {Delivering on the {{Vision}} of {{MLOps}}},
  author = {Collins, Jon},
  pages = {33},
  file = {/Users/antonio/Zotero/storage/HDH6GDVY/Collins - Delivering on the Vision of MLOps.pdf},
  language = {en}
}

@misc{CometBuildBetter,
  title = {Comet \textendash{} {{Build}} Better Models Faster!},
  file = {/Users/antonio/Zotero/storage/HDXCMW2Y/site.html},
  language = {en-US}
}

@article{Cortes2004SupportVectorN,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, V.},
  year = {2004},
  volume = {20},
  pages = {273--297},
  journal = {Machine Learning}
}

@article{cortesSupportvectorNetworks1995,
  title = {Support-Vector Networks},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  year = {1995},
  month = sep,
  volume = {20},
  pages = {273--297},
  issn = {1573-0565},
  doi = {10.1007/BF00994018},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
  file = {/Users/antonio/Zotero/storage/KLSUBG3V/Cortes and Vapnik - 1995 - Support-vector networks.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {3}
}

@misc{CUDAToolkit2013,
  title = {{{CUDA Toolkit}}},
  year = {2013},
  month = jul,
  abstract = {CUDA Toolkit Develop, Optimize and Deploy GPU-Accelerated Apps The NVIDIA\textregistered{} CUDA\textregistered{} Toolkit provides a development environment for creating high performance GPU-accelerated applications. With the CUDA Toolkit, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms and HPC supercomputers. The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library to deploy your application.},
  file = {/Users/antonio/Zotero/storage/WD49X2WT/cuda-toolkit.html},
  howpublished = {https://developer.nvidia.com/cuda-toolkit},
  journal = {NVIDIA Developer},
  language = {en}
}

@misc{DataGripIDEMultiplataforma,
  title = {{DataGrip: el IDE multiplataforma para bases de datos y SQL, de JetBrains}},
  shorttitle = {{DataGrip}},
  abstract = {Un potente IDE de JetBrains para SQL en macOS, Windows y Linux.},
  file = {/Users/antonio/Zotero/storage/L79CNF68/datagrip.html},
  howpublished = {https://www.jetbrains.com/es-es/datagrip/},
  journal = {JetBrains},
  language = {es}
}

@misc{DataScienceCollaboration,
  title = {Data Science Collaboration Hub.},
  abstract = {Neptune is an experiment tracking tool bringing organization and collaboration to data science projects. Exploratory notebooks, model training runs, code, hyperparameters, metrics, data versions, results exploration visualizations and more. Everything is safely stored, ready to be analyzed, shared and discussed with your team.},
  file = {/Users/antonio/Zotero/storage/ZM5LZB65/neptune.ai.html},
  howpublished = {https://neptune.ai/},
  journal = {neptune.ai},
  language = {en-US}
}

@article{degrangeIntroductionHighenergyGammaray2015,
  title = {Introduction to High-Energy Gamma-Ray Astronomy},
  author = {Degrange, Bernard and Fontaine, G{\'e}rard},
  year = {2015},
  month = aug,
  volume = {16},
  pages = {587--599},
  issn = {16310705},
  doi = {10.1016/j.crhy.2015.07.003},
  abstract = {The present issue is the first of of a two-volume review devoted to gamma-ray astronomy above 100 MeV which has witnessed considerable progress over the last 20 years. The motivations for research in this area are explained, the follow-on articles of these two thematic issues are introduced and a brief history of the field is given.},
  archivePrefix = {arXiv},
  eprint = {1604.05488},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/JPYBDW9J/Degrange and Fontaine - 2015 - Introduction to high-energy gamma-ray astronomy.pdf;/Users/antonio/Zotero/storage/23UCS829/1604.html},
  journal = {Comptes Rendus Physique},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics},
  number = {6-7}
}

@inproceedings{demmelLAPACKPortableLinear1989,
  title = {{{LAPACK}}: A Portable Linear Algebra Library for Supercomputers},
  shorttitle = {{{LAPACK}}},
  booktitle = {{{IEEE Control Systems Society Workshop}} on {{Computer}}-{{Aided Control System Design}}},
  author = {Demmel, J.},
  year = {1989},
  month = dec,
  pages = {1--7},
  doi = {10.1109/CACSD.1989.69824},
  abstract = {A portable FORTRAN linear algebra library, LAPACK for efficient use on a variety of high-performance computers is discussed. The library is based on the widely used LINPACK and EISPACK packages for linear equation solving, eigenvalue problems, and linear least squares, but it extends their functionality in a number of ways. The major methodology for making the algorithms run faster is to restructure them to perform block matrix operations (like matrix-matrix multiplication) in their inner loops. These block operations can be optimized to exploit the memory hierarchy of different architectures, resulting in large speedups. Divide-and-conquer algorithms are used for a variety of eigenproblems. Algorithms which are much more accurate than their conventional counterparts for a variety of problems are described.{$<>$}},
  file = {/Users/antonio/Zotero/storage/SJ3K593J/69824.html},
  keywords = {block matrix operations,Computer architecture,divide-and-conquer algorithms,eigenproblems,eigenvalue problems,Eigenvalues and eigenfunctions,EISPACK,Equations,inner loops,LAPACK,Least squares methods,Libraries,linear algebra,Linear algebra,linear equation solving,linear least squares,LINPACK,mathematics computing,memory hierarchy,Packaging machines,Portable computers,portable FORTRAN linear algebra library,software portability,subroutines,supercomputers,Supercomputers,Vectors}
}

@misc{DessaossAtlas2020,
  title = {Dessa-Oss/Atlas},
  year = {2020},
  month = jun,
  abstract = {An Open Source, Self-Hosted Platform For Applied Deep Learning Development},
  copyright = {Apache-2.0},
  howpublished = {Dessa - Open Source},
  keywords = {ai,artificial-intelligence,data-science,deep-learning,gpu,machine-learning,ml,model-management,python}
}

@misc{domenechAntoniomdkSeminarsTalks2020,
  title = {Antoniomdk/Seminars\_and\_talks},
  author = {Domenech, Antonio Molner},
  year = {2020},
  month = apr,
  abstract = {Example code and slides from seminars and other technical talks I've given}
}

@misc{EdgeOrg,
  title = {Edge.Org},
  file = {/Users/antonio/Zotero/storage/VTFU7HLL/25340.html},
  howpublished = {https://www.edge.org/response-detail/25340}
}

@inproceedings{freireComputationalReproducibilityStateoftheart2012,
  title = {Computational Reproducibility: State-of-the-Art, Challenges, and Database Research Opportunities},
  shorttitle = {Computational Reproducibility},
  booktitle = {Proceedings of the 2012 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author = {Freire, Juliana and Bonnet, Philippe and Shasha, Dennis},
  year = {2012},
  month = may,
  pages = {593--596},
  publisher = {{Association for Computing Machinery}},
  address = {{Scottsdale, Arizona, USA}},
  doi = {10.1145/2213836.2213908},
  abstract = {Computational experiments have become an integral part of the scientific method, but reproducing, archiving, and querying them is still a challenge. The first barrier to a wider adoption is the fact that it is hard both for authors to derive a compendium that encapsulates all the components needed to reproduce a result and for reviewers to verify the results. In this tutorial, we will present a series of guidelines and, through hands-on examples, review existing tools to help authors create of reproducible results. We will also outline open problems and new directions for database-related research having to do with querying computational experiments.},
  file = {/Users/antonio/Zotero/storage/2G23E8U7/Freire et al. - 2012 - Computational reproducibility state-of-the-art, c.pdf},
  isbn = {978-1-4503-1247-9},
  keywords = {computational reproducibility,reproducible publications},
  series = {{{SIGMOD}} '12}
}

@article{freireReproducibilityDataOrientedExperiments2016,
  title = {Reproducibility of {{Data}}-{{Oriented Experiments}} in e-{{Science}} ({{Dagstuhl Seminar}} 16041)},
  author = {Freire, Juliana and Fuhr, Norbert and Rauber, Andreas},
  editor = {Freire, Juliana and Fuhr, Norbert and Rauber, Andreas},
  year = {2016},
  volume = {6},
  pages = {108--159},
  publisher = {{Schloss Dagstuhl\textendash Leibniz-Zentrum fuer Informatik}},
  address = {{Dagstuhl, Germany}},
  issn = {2192-5283},
  doi = {10.4230/DagRep.6.1.108},
  file = {/Users/antonio/Zotero/storage/RAE9F9QD/Freire et al. - 2016 - Reproducibility of Data-Oriented Experiments in e-.pdf;/Users/antonio/Zotero/storage/3U5X44VT/5817.html},
  journal = {Dagstuhl Reports},
  keywords = {Documentation,Reliability,Repeatibility,Replicability,reproducibility,Software},
  number = {1}
}

@article{Friedman2001GreedyFA,
  title = {Greedy Function Approximation: {{A}} Gradient Boosting Machine.},
  author = {Friedman, J.},
  year = {2001},
  volume = {29},
  pages = {1189--1232},
  journal = {Annals of Statistics}
}

@article{friedmanGreedyFunctionApproximation2001,
  title = {Greedy {{Function Approximation}}: {{A Gradient Boosting Machine}}},
  shorttitle = {Greedy {{Function Approximation}}},
  author = {Friedman, Jerome H.},
  year = {2001},
  volume = {29},
  pages = {1189--1232},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  journal = {The Annals of Statistics},
  number = {5}
}

@book{geronHandsOnMachineLearning2019,
  title = {{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems}},
  shorttitle = {{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2019},
  month = sep,
  edition = {Edici\'on: 2},
  publisher = {{O'Reilly Media}},
  abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how.By using concrete examples, minimal theory, and two production-ready Python frameworks\textemdash Scikit-Learn and TensorFlow\textemdash author Aur\'elien G\'eron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started.Explore the machine learning landscape, particularly neural netsUse Scikit-Learn to track an example machine-learning project end-to-endExplore several training models, including support vector machines, decision trees, random forests, and ensemble methodsUse the TensorFlow library to build and train neural netsDive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learningLearn techniques for training and scaling deep neural nets},
  language = {Ingl\'es}
}

@book{geronHandsOnMachineLearning2019a,
  title = {{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.}},
  shorttitle = {{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2019},
  month = sep,
  edition = {Edici\'on: 2},
  publisher = {{O'Reilly Media}},
  abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how.By using concrete examples, minimal theory, and two production-ready Python frameworks\textemdash Scikit-Learn and TensorFlow\textemdash author Aur\'elien G\'eron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started.Explore the machine learning landscape, particularly neural netsUse Scikit-Learn to track an example machine-learning project end-to-endExplore several training models, including support vector machines, decision trees, random forests, and ensemble methodsUse the TensorFlow library to build and train neural netsDive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learningLearn techniques for training and scaling deep neural nets},
  language = {Ingl\'es}
}

@article{gibneyThisAIResearcher2019,
  title = {This {{AI}} Researcher Is Trying to Ward off a Reproducibility Crisis},
  author = {Gibney, Elizabeth},
  year = {2019},
  month = dec,
  volume = {577},
  pages = {14--14},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-03895-5},
  abstract = {Joelle Pineau is leading an effort to encourage artificial-intelligence researchers to open up their code.},
  copyright = {2020 Nature},
  file = {/Users/antonio/Zotero/storage/9NEKI9QX/Gibney - 2019 - This AI researcher is trying to ward off a reprodu.pdf;/Users/antonio/Zotero/storage/AI9FZZI5/d41586-019-03895-5.html},
  journal = {Nature},
  language = {en},
  number = {7788}
}

@article{guillenDeepLearningTechniques2019,
  title = {Deep Learning Techniques Applied to the Physics of Extensive Air Showers},
  author = {Guillen, A. and Bueno, A. and Carceller, J. M. and {Martinez-Velazquez}, J. C. and Rubio, G. and Peixoto, C. J. Todero and {Sanchez-Lucas}, P.},
  year = {2019},
  month = sep,
  volume = {111},
  pages = {12--22},
  issn = {09276505},
  doi = {10.1016/j.astropartphys.2019.03.001},
  abstract = {Deep neural networks are a powerful technique that have found ample applications in several branches of Physics. In this work, we apply machine learning algorithms to a specific problem of Cosmic Ray Physics: the estimation of the muon content of extensive air showers when measured at the ground. As a working case, we explore the performance of a deep neural network applied to the signals recorded by the water-Cherenkov detectors of the Surface Detector Array of the Pierre Auger Observatory. We apply deep learning architectures to large sets of simulated data. The inner structure of the neural network is optimized through the use of genetic algorithms. To obtain a prediction of the recorded muon signal in each individual detector, we train neural networks with a mixed sample of light, intermediate and heavy nuclei. When true and predicted signals are compared at detector level, the primary values of the Pearson correlation coefficients are above 95\textbackslash\%. The relative errors of the predicted muon signals are below 10\textbackslash\% and do not depend on the event energy, zenith angle, total signal size, distance range or the hadronic model used to generate the events.},
  archivePrefix = {arXiv},
  eprint = {1807.09024},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/WAJ8FCBR/Guillen et al. - 2019 - Deep learning techniques applied to the physics of.pdf;/Users/antonio/Zotero/storage/CLZ4CR8Y/1807.html},
  journal = {Astroparticle Physics},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics}
}

@book{gulliDeepLearningKeras2017,
  title = {Deep {{Learning}} with {{Keras}}},
  author = {Gulli, Antonio and Pal, Sujit},
  year = {2017},
  month = apr,
  publisher = {{Packt Publishing Ltd}},
  abstract = {Get to grips with the basics of Keras to implement fast and efficient deep-learning modelsAbout This BookImplement various deep-learning algorithms in Keras and see how deep-learning can be used in gamesSee how various deep-learning models and practical use-cases can be implemented using KerasA practical, hands-on guide with real-world examples to give you a strong foundation in KerasWho This Book Is ForIf you are a data scientist with experience in machine learning or an AI programmer with some exposure to neural networks, you will find this book a useful entry point to deep-learning with Keras. A knowledge of Python is required for this book.What You Will LearnOptimize step-by-step functions on a large neural network using the Backpropagation AlgorithmFine-tune a neural network to improve the quality of resultsUse deep learning for image and audio processingUse Recursive Neural Tensor Networks (RNTNs) to outperform standard word embedding in special casesIdentify problems for which Recurrent Neural Network (RNN) solutions are suitableExplore the process required to implement AutoencodersEvolve a deep neural network using reinforcement learningIn DetailThis book starts by introducing you to supervised learning algorithms such as simple linear regression, the classical multilayer perceptron and more sophisticated deep convolutional networks. You will also explore image processing with recognition of hand written digit images, classification of images into different categories, and advanced objects recognition with related image annotations. An example of identification of salient points for face detection is also provided. Next you will be introduced to Recurrent Networks, which are optimized for processing sequence data such as text, audio or time series. Following that, you will learn about unsupervised learning algorithms such as Autoencoders and the very popular Generative Adversarial Networks (GAN). You will also explore non-traditional uses of neural networks as Style Transfer.Finally, you will look at Reinforcement Learning and its application to AI game playing, another popular direction of research and application of neural networks.Style and approachThis book is an easy-to-follow guide full of examples and real-world applications to help you gain an in-depth understanding of Keras. This book will showcase more than twenty working Deep Neural Networks coded in Python using Keras.},
  googlebooks = {20EwDwAAQBAJ},
  isbn = {978-1-78712-903-0},
  keywords = {Computers / Data Processing,Computers / Intelligence (AI) \& Semantics,Computers / Neural Networks},
  language = {en}
}

@incollection{hanoldProfilingAutomatedDecisionMaking2018,
  title = {Profiling and {{Automated Decision}}-{{Making}}: {{Legal Implications}} and {{Shortcomings}}},
  shorttitle = {Profiling and {{Automated Decision}}-{{Making}}},
  booktitle = {Robotics, {{AI}} and the {{Future}} of {{Law}}},
  author = {H{\"a}nold, Stefanie},
  editor = {Corrales, Marcelo and Fenwick, Mark and Forg{\'o}, Nikolaus},
  year = {2018},
  pages = {123--153},
  publisher = {{Springer}},
  address = {{Singapore}},
  doi = {10.1007/978-981-13-2874-9_6},
  abstract = {The increased use of profiling and automated decision-making systems raises a number of challenges and concerns. The underlying algorithms embody a considerable potential for discrimination and unfair treatment. Furthermore, individuals are treated as passive objects of algorithmic evaluation and decision tools and are unable to present their values and positions. They are no longer perceived as individuals in their own right: all that matters is the group they are assigned to. Profiling and automated decision-making techniques also depend on the processing of personal data, and a significant number of the available applications are highly privacy-intrusive. This article analyses how the European General Data Protection Regulation (GDPR) responds to these challenges. In particular, Art. 22 GDPR, which provides the right not to be subject to automated individual decision-making, as well as the information obligations under Art. 13 (2) (f) and Art. 14 (2) (g) GDPR and the access right under Art. 15 (1) (h) GDPR, will be examined in detail. General data protection principles, particularly the principle of fairness, as well as specific German scoring provisions and anti-discrimination rules, are looked at, too. In conclusion, various shortcomings of the present legal framework are identified and discussed and a short outlook for potential future steps presented.},
  file = {/Users/antonio/Zotero/storage/5SQD2BYV/Hänold - 2018 - Profiling and Automated Decision-Making Legal Imp.pdf},
  isbn = {9789811328749},
  keywords = {Algorithm,Automated decision-making,Explanation,General Data Protection Regulation (GDPR),Profiling},
  language = {en},
  series = {Perspectives in {{Law}}, {{Business}} and {{Innovation}}}
}

@misc{HdbscanClusteringLibrary,
  title = {The Hdbscan {{Clustering Library}} \textemdash{} Hdbscan 0.8.1 Documentation},
  file = {/Users/antonio/Zotero/storage/LRPINHLU/latest.html},
  howpublished = {https://hdbscan.readthedocs.io/en/latest/}
}

@article{headExtentConsequencesPHacking2015,
  title = {The {{Extent}} and {{Consequences}} of {{P}}-{{Hacking}} in {{Science}}},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  month = mar,
  volume = {13},
  pages = {e1002106},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002106},
  abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as ``p-hacking,'' occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
  file = {/Users/antonio/Zotero/storage/D3D4EEWR/Head et al. - 2015 - The Extent and Consequences of P-Hacking in Scienc.pdf;/Users/antonio/Zotero/storage/3JJGPB6M/article.html},
  journal = {PLOS Biology},
  keywords = {Binomials,Medicine and health sciences,Meta-analysis,Psychology,Publication ethics,Research validity,Statistical data,Test statistics},
  language = {en},
  number = {3}
}

@incollection{hecht-nielsenIIITheoryBackpropagation1992,
  title = {{{III}}.3 - {{Theory}} of the {{Backpropagation Neural Network}}**{{Based}} on ``Nonindent'' by {{Robert Hecht}}-{{Nielsen}}, Which Appeared in {{Proceedings}} of the {{International Joint Conference}} on {{Neural Networks}} 1, 593\textendash 611, {{June}} 1989. \textcopyright{} 1989 {{IEEE}}.},
  booktitle = {Neural {{Networks}} for {{Perception}}},
  author = {{Hecht-nielsen}, ROBERT},
  editor = {Wechsler, Harry},
  year = {1992},
  month = jan,
  pages = {65--93},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-741252-8.50010-8},
  abstract = {This chapter presents a survey of the elementary theory of the basic backpropagation neural network architecture, covering the areas of architectural design, performance measurement, function approximation capability, and learning. The survey includes a formulation of the backpropagation neural network architecture to make it a valid neural network and a proof that the backpropagation mean squared error function exists and is differentiable. Also included in the survey is a theorem showing that any L2 function can be implemented to any desired degree of accuracy with a three-layer backpropagation neural network. An appendix presents a speculative neurophysiological model illustrating the way in which the backpropagation neural network architecture might plausibly be implemented in the mammalian brain for corticocortical learning between nearby regions of cerebral cortex. One of the crucial decisions in the design of the backpropagation architecture is the selection of a sigmoidal activation function.},
  file = {/Users/antonio/Zotero/storage/NR9H3XQQ/B9780127412528500108.html},
  isbn = {978-0-12-741252-8},
  language = {en}
}

@article{heckCORSIKAMonteCarlo1998,
  title = {{{CORSIKA}}: {{A Monte Carlo}} Code to Simulate Extensive Air Showers},
  shorttitle = {{{CORSIKA}}},
  author = {Heck, D. and Knapp, J. and Capdevielle, J. N. and Schatz, G. and Thouw, T.},
  year = {1998},
  month = feb,
  file = {/Users/antonio/Zotero/storage/8GVPJF2U/Heck et al. - 1998 - CORSIKA A Monte Carlo code to simulate extensive .pdf;/Users/antonio/Zotero/storage/A8CA6VA5/469835.html},
  language = {en}
}

@article{hewittActorModelComputation2015,
  title = {Actor {{Model}} of {{Computation}}: {{Scalable Robust Information Systems}}},
  shorttitle = {Actor {{Model}} of {{Computation}}},
  author = {Hewitt, Carl},
  year = {2015},
  month = jan,
  abstract = {The Actor model is a mathematical theory that treats "Actors" as the universal primitives of concurrent digital computation. The model has been used both as a framework for a theoretical understanding of concurrency, and as the theoretical basis for several practical implementations of concurrent systems. Unlike previous models of computation, the Actor model was inspired by physical laws. It was also influenced by the programming languages Lisp, Simula 67 and Smalltalk-72, as well as ideas for Petri Nets, capability-based systems and packet switching. The advent of massive concurrency through client-cloud computing and many-core computer architectures has galvanized interest in the Actor model. Actor technology will see significant application for integrating all kinds of digital information for individuals, groups, and organizations so their information usefully links together. Information integration needs to make use of the following information system principles: * Persistence. Information is collected and indexed. * Concurrency: Work proceeds interactively and concurrently, overlapping in time. * Quasi-commutativity: Information can be used regardless of whether it initiates new work or become relevant to ongoing work. * Sponsorship: Sponsors provide resources for computation, i.e., processing, storage, and communications. * Pluralism: Information is heterogeneous, overlapping and often inconsistent. * Provenance: The provenance of information is carefully tracked and recorded The Actor Model is intended to provide a foundation for inconsistency robust information integration},
  archivePrefix = {arXiv},
  eprint = {1008.1459},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/HMWIARFP/Hewitt - 2015 - Actor Model of Computation Scalable Robust Inform.pdf;/Users/antonio/Zotero/storage/UWBYF7DJ/1008.html},
  journal = {arXiv:1008.1459 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Programming Languages},
  primaryClass = {cs}
}

@techreport{hintzeAutomatedIndividualDecisions2020,
  title = {Automated {{Individual Decisions}} to {{Disclose Personal Data}}: {{Why GDPR Article}} 22 {{Should Not Apply}}},
  shorttitle = {Automated {{Individual Decisions}} to {{Disclose Personal Data}}},
  author = {Hintze, Mike},
  year = {2020},
  month = jun,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3630026},
  abstract = {"The data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her."  This opening paragraph of Article 22 of the EU General Data Protection Regulation (GDPR) describes the types of automated decision-making that are subject to special restrictions and requirements under the GDPR.  But despite incorporating principles that already existed in European data protection law,  and despite guidance having been published by European data protection authorities,  Article 22 continues to be a source of great uncertainty.   Organizations of all types are increasingly adopting the tools of machine learning and artificial intelligence in a variety of applications. Such organizations must determine when and how the Article 22 restrictions on automated decision-making apply.  Depending on whether Article 22 applies broadly or narrowly will have dramatic impacts on a wide range of organizations. Overly broad interpretations will likely result in less efficient and more costly operations and could lead to less accurate and reliable decision-making.  A narrower interpretation, on the other hand, will apply the unique protections afforded by Article 22 to the type of decision-making for which those protections will be meaningful, while other types of decision-making continue to be subject to the full range of protections that other provisions of the GDPR afford.This paper will provide an overview of Article 22 and will examine several considerations that are important for determining its scope.  It will argue that the scope of automated decision-making regulated by Article 22 is quite narrow, limited to those solely automated decisions where a legal or similarly significant effect is an inherent and direct result of the decision and where human intervention could be helpful and meaningful in protecting individual rights. It will then use those considerations and conclusions to discuss one type of automated decision-making to determine whether it should be regulated by Article 22.  Specifically, it will ask whether an automated decision to disclose personal data to a third party is subject to Article 22. This paper will demonstrate that in most cases, if not virtually every case, automated decisions to disclose personal data are not of the type contemplated by, or appropriate for, the application of Article 22. While acknowledging that any decision to disclose personal data raises significant privacy and data protection concerns, this paper will argue that other protections afforded by the GDPR are better to suited address those concerns and safeguard individual rights.  Thus, this paper will conclude that, as a general matter, Article 22 should be interpreted to exclude such automated decisions.},
  file = {/Users/antonio/Zotero/storage/4TQ93F2T/papers.html},
  keywords = {AI,artificial intelligence,automated decision-making,data protection,GDPR,machine learning,privacy,profiling},
  language = {en},
  number = {ID 3630026},
  type = {{{SSRN Scholarly Paper}}}
}

@article{howardFastaiLayeredAPI2020,
  title = {Fastai: {{A Layered API}} for {{Deep Learning}}},
  shorttitle = {Fastai},
  author = {Howard, Jeremy and Gugger, Sylvain},
  year = {2020},
  month = feb,
  volume = {11},
  pages = {108},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/info11020108},
  abstract = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4\&ndash;5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/Users/antonio/Zotero/storage/6W7JYWVP/Howard and Gugger - 2020 - Fastai A Layered API for Deep Learning.pdf;/Users/antonio/Zotero/storage/EFPKJGKR/108.html},
  journal = {Information},
  keywords = {data processing pipelines,deep learning},
  language = {en},
  number = {2}
}

@article{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archivePrefix = {arXiv},
  eprint = {1704.04861},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/7Q84YSBF/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;/Users/antonio/Zotero/storage/S4W9HJLY/1704.html},
  journal = {arXiv:1704.04861 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{hutsonAIGlossaryArtificial2017,
  title = {{{AI Glossary}}: {{Artificial}} Intelligence, in so Many Words},
  shorttitle = {{{AI Glossary}}},
  author = {Hutson, Matthew},
  year = {2017},
  month = jul,
  volume = {357},
  pages = {19--19},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.357.6346.19},
  abstract = {Just what do people mean by artificial intelligence (AI)? The term has never had clear boundaries. When it was introduced at a seminal 1956 workshop at Dartmouth College, it was taken broadly to mean making a machine behave in ways that would be called intelligent if seen in a human. An important recent advance in AI has been machine learning, which shows up in technologies from spellcheck to self-driving cars and is often carried out by computer systems called neural networks. Any discussion of AI is likely to include other terms as well. We present a glossary of key words and phrases. Defining the terms of artificial intelligence Defining the terms of artificial intelligence},
  chapter = {News},
  copyright = {Copyright \textcopyright{} 2017 The Authors, some rights reserved;                     exclusive licensee American Association for the Advancement of Science. No claim                     to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  file = {/Users/antonio/Zotero/storage/3F5FUR5N/Hutson - 2017 - AI Glossary Artificial intelligence, in so many w.pdf;/Users/antonio/Zotero/storage/KFMGRT3W/tab-pdf.html},
  journal = {Science},
  language = {en},
  number = {6346},
  pmid = {28684481}
}

@misc{IDSIASacred2020,
  title = {{{IDSIA}}/Sacred},
  year = {2020},
  month = jun,
  abstract = {Sacred is a tool to help you configure, organize, log and reproduce experiments developed at IDSIA.},
  copyright = {MIT},
  howpublished = {IDSIA},
  keywords = {infrastructure,machine-learning,mongodb,python,reproducibility,reproducible-research,reproducible-science}
}

@misc{IDSIASacred2020a,
  title = {{{IDSIA}}/Sacred},
  year = {2020},
  month = jun,
  abstract = {Sacred is a tool to help you configure, organize, log and reproduce experiments developed at IDSIA.},
  copyright = {MIT},
  howpublished = {IDSIA},
  keywords = {infrastructure,machine-learning,mongodb,python,reproducibility,reproducible-research,reproducible-science}
}

@misc{IterativeDvc2020,
  title = {Iterative/Dvc},
  year = {2020},
  month = jun,
  abstract = {🦉Data Version Control | Git for Data \& Models. Contribute to iterative/dvc development by creating an account on GitHub.},
  copyright = {Apache-2.0},
  howpublished = {Iterative},
  keywords = {ai,collaboration,data-science,data-version-control,developer-tools,git,hacktoberfest,machine-learning,python,reproducibility}
}

@inproceedings{juDeepLearningMethod2015,
  title = {A {{Deep Learning Method Combined Sparse Autoencoder}} with {{SVM}}},
  booktitle = {2015 {{International Conference}} on {{Cyber}}-{{Enabled Distributed Computing}} and {{Knowledge Discovery}}},
  author = {Ju, Yao and Guo, Jun and Liu, Shuchun},
  year = {2015},
  month = sep,
  pages = {257--260},
  doi = {10.1109/CyberC.2015.39},
  abstract = {In this paper, a novel unsupervised method for learning sparse features combined with support vector machines for classification is proposed. The classical SVM method has restrictions on the large-scale applications. This model uses sparse auto encoder, a deep learning algorithm, to improve the performance. Firstly, we use multiple layers of sparse auto encoder to learn the features of the data. Secondly, we use SVM to classify. Many experimental results show that compared with SVM, our proposed method can improve the classification rate. In particular, it can effectively deal with large-scale data sets.},
  file = {/Users/antonio/Zotero/storage/V67CIUIF/7307823.html},
  keywords = {Classification algorithms,data classification,Data models,deep learning,deep learning algorithm,encoding,Feature extraction,Kernel,Machine learning,pattern classification,sparse autoencoder,support vector machine,support vector machines,Support vector machines,SVM,Training,unsupervised learning,unsupervised learning method}
}

@article{kanellopoulou-bottiRightHumanIntervention2019,
  title = {The {{Right}} to {{Human Intervention}}: {{Law}}, {{Ethics}} and {{Artificial Intelligence}}},
  shorttitle = {The {{Right}} to {{Human Intervention}}},
  author = {{Kanellopoulou - Botti}, Maria and Panagopoulou, Fereniki and Nikita, Maria and Michailaki, Anastasia},
  year = {2019},
  month = may,
  volume = {2019},
  issn = {2689-2782},
  doi = {10.25884/er01-fe46},
  journal = {Computer Ethics - Philosophical Enquiry (CEPE) Proceedings},
  number = {1}
}

@article{kearns2013machine,
  title = {Machine Learning for Market Microstructure and High Frequency Trading},
  author = {Kearns, Michael and Nevmyvaka, Yuriy},
  year = {2013},
  journal = {High Frequency Trading: New Realities for Traders, Markets, and Regulators}
}

@incollection{keLightGBMHighlyEfficient2017,
  title = {{{LightGBM}}: {{A Highly Efficient Gradient Boosting Decision Tree}}},
  shorttitle = {{{LightGBM}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {3146--3154},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/antonio/Zotero/storage/A8WF8SV8/6907-lightgbm-a-highly-efficient-gradient-boosting-decision.html}
}

@book{kelleherDataScience2018,
  title = {Data {{Science}}},
  author = {Kelleher, John D. and Tierney, Brendan},
  year = {2018},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/11140.001.0001},
  isbn = {978-0-262-34702-0},
  language = {en}
}

@inbook{kelleherSTANDARDDATASCIENCE2018,
  title = {{{STANDARD DATA SCIENCE TASKS}}},
  booktitle = {Data {{Science}}},
  year = {2018},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/11140.003.0009},
  collaborator = {Kelleher, John D. and Tierney, Brendan},
  isbn = {978-0-262-34702-0},
  language = {en}
}

@misc{KerasPythonDeep,
  title = {Keras: The {{Python}} Deep Learning {{API}}},
  file = {/Users/antonio/Zotero/storage/F4QN5NU5/keras.io.html},
  howpublished = {https://keras.io/}
}

@article{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/69RJW5PH/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;/Users/antonio/Zotero/storage/DXMWYUN7/1312.html},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  volume = {12},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archivePrefix = {arXiv},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/IY3MWS5K/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/Users/antonio/Zotero/storage/URXQGQRY/1906.html},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  number = {4}
}

@article{kingmaSemiSupervisedLearningDeep2014,
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  year = {2014},
  month = oct,
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1406.5298},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/ZFDSXEDN/Kingma et al. - 2014 - Semi-Supervised Learning with Deep Generative Mode.pdf;/Users/antonio/Zotero/storage/5H8ICRF4/1406.html},
  journal = {arXiv:1406.5298 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{kohaviImprovingSimpleBayes1997,
  title = {Improving {{Simple Bayes}}},
  author = {Kohavi, Ron and Becker, Barry and Sommerfield, Dan},
  year = {1997},
  abstract = {. The simple Bayesian classifier (SBC), sometimes called Naive-Bayes, is built based on a conditional independence model of each attribute given the class. The model was previously shown to be surprisingly robust to obvious violations of this independence assumption, yielding accurate classification models even when there are clear conditional dependencies. We examine different approaches for handling unknowns and zero counts when estimating probabilities. Large scale experiments on 37 datasets were conducted to determine the effects of these approaches and several interesting insights are given, including a new variant of the Laplace estimator that outperforms other methods for dealing with zero counts. Using the bias-variance decomposition [15, 10], we show that while the SBC has performed well on common benchmark datasets, its accuracy will not scale up as the dataset sizes grow. Even with these limitations in mind, the SBC can serve as an excellent tool for initial exploratory data...},
  file = {/Users/antonio/Zotero/storage/Z7QV9CEI/Kohavi et al. - 1997 - Improving Simple Bayes.pdf;/Users/antonio/Zotero/storage/3TVYXFFF/summary.html}
}

@article{kohaviScalingTheaADceccuisriaocnyTorfeNeaHivyebBridayesClassi,
  title = {Scaling {{Up theaADceccuisriaocny}}-{{TorfeNeaHivyeb}}-{{Bridayes Classi}} Ers:},
  author = {Kohavi, Ron},
  pages = {6},
  abstract = {Naive-Bayes induction algorithms were previously shown to be surprisingly accurate on many classi cation tasks even when the conditional independence assumption on which they are based is violated. However, most studies were done on small databases. We show that in some larger databases, the accuracy of Naive-Bayes does not scale up as well as decision trees. We then propose a new algorithm, NBTree, which induces a hybrid of decision-tree classi ers and NaiveBayes classi ers: the decision-tree nodes contain univariate splits as regular decision-trees, but the leaves contain Naive-Bayesian classi ers. The approach retains the interpretability of Naive-Bayes and decision trees, while resulting in classi ers that frequently outperform both constituents, especially in the larger databases tested.},
  file = {/Users/antonio/Zotero/storage/SZWDCUNE/Kohavi - Scaling Up theaADceccuisriaocny-TorfeNeaHivyeb-Bri.pdf},
  language = {en}
}

@article{komorowskiDeepBallDeepNeuralNetwork2019,
  title = {{{DeepBall}}: {{Deep Neural}}-{{Network Ball Detector}}},
  shorttitle = {{{DeepBall}}},
  author = {Komorowski, Jacek and Kurzejamski, Grzegorz and Sarwas, Grzegorz},
  year = {2019},
  pages = {297--304},
  doi = {10.5220/0007348902970304},
  abstract = {The paper describes a deep network based object detector specialized for ball detection in long shot videos. Due to its fully convolutional design, the method operates on images of any size and produces \textbackslash emph\{ball confidence map\} encoding the position of detected ball. The network uses hypercolumn concept, where feature maps from different hierarchy levels of the deep convolutional network are combined and jointly fed to the convolutional classification layer. This allows boosting the detection accuracy as larger visual context around the object of interest is taken into account. The method achieves state-of-the-art results when tested on publicly available ISSIA-CNR Soccer Dataset.},
  archivePrefix = {arXiv},
  eprint = {1902.07304},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/DJIJ6PET/Komorowski et al. - 2019 - DeepBall Deep Neural-Network Ball Detector.pdf},
  journal = {Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{koudasRecordLinkageSimilarity2006,
  title = {Record Linkage: Similarity Measures and Algorithms},
  shorttitle = {Record Linkage},
  booktitle = {Proceedings of the 2006 {{ACM SIGMOD}} International Conference on {{Management}} of Data},
  author = {Koudas, Nick and Sarawagi, Sunita and Srivastava, Divesh},
  year = {2006},
  month = jun,
  pages = {802--803},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1142473.1142599},
  abstract = {This tutorial provides a comprehensive and cohesive overview of the key research results in the area of record linkage methodologies and algorithms for identifying approximate duplicate records, and available tools for this purpose. It encompasses techniques introduced in several communities including databases, information retrieval, statistics and machine learning. It aims to identify similarities and differences across the techniques as well as their merits and limitations.},
  isbn = {978-1-59593-434-5},
  keywords = {approximate join,data quality},
  series = {{{SIGMOD}} '06}
}

@book{kraghQuantumGenerationsHistory2002,
  title = {Quantum {{Generations}}: {{A History}} of {{Physics}} in the {{Twentieth Century}}},
  shorttitle = {Quantum {{Generations}}},
  author = {Kragh, Helge},
  year = {2002},
  month = mar,
  publisher = {{Princeton University Press}},
  abstract = {At the end of the nineteenth century, some physicists believed that the basic principles underlying their subject were already known, and that physics in the future would only consist of filling in the details. They could hardly have been more wrong. The past century has seen the rise of quantum mechanics, relativity, cosmology, particle physics, and solid-state physics, among other fields. These subjects have fundamentally changed our understanding of space, time, and matter. They have also transformed daily life, inspiring a technological revolution that has included the development of radio, television, lasers, nuclear power, and computers. In Quantum Generations, Helge Kragh, one of the world's leading historians of physics, presents a sweeping account of these extraordinary achievements of the past one hundred years. The first comprehensive one-volume history of twentieth-century physics, the book takes us from the discovery of X rays in the mid-1890s to superstring theory in the 1990s. Unlike most previous histories of physics, written either from a scientific perspective or from a social and institutional perspective, Quantum Generations combines both approaches. Kragh writes about pure science with the expertise of a trained physicist, while keeping the content accessible to nonspecialists and paying careful attention to practical uses of science, ranging from compact disks to bombs. As a historian, Kragh skillfully outlines the social and economic contexts that have shaped the field in the twentieth century. He writes, for example, about the impact of the two world wars, the fate of physics under Hitler, Mussolini, and Stalin, the role of military research, the emerging leadership of the United States, and the backlash against science that began in the 1960s. He also shows how the revolutionary discoveries of scientists ranging from Einstein, Planck, and Bohr to Stephen Hawking have been built on the great traditions of earlier centuries. Combining a mastery of detail with a sure sense of the broad contours of historical change, Kragh has written a fitting tribute to the scientists who have played such a decisive role in the making of the modern world.},
  googlebooks = {ELrFDIldlawC},
  isbn = {978-0-691-09552-3},
  keywords = {Science / History,Science / Physics / General},
  language = {en}
}

@article{kruchtenTechnicalDebtMetaphor2012,
  title = {Technical {{Debt}}: {{From Metaphor}} to {{Theory}} and {{Practice}}},
  shorttitle = {Technical {{Debt}}},
  author = {Kruchten, Philippe and Nord, Robert L. and Ozkaya, Ipek},
  year = {2012},
  month = nov,
  volume = {29},
  pages = {18--21},
  issn = {1937-4194},
  doi = {10.1109/MS.2012.167},
  abstract = {The metaphor of technical debt in software development was introduced two decades ago to explain to nontechnical stakeholders the need for what we call now "refactoring." As the term is being used to describe a wide range of phenomena, this paper proposes an organization of the technical debt landscape, and introduces the papers on technical debt contained in the issue.},
  file = {/Users/antonio/Zotero/storage/TGVQVV7X/6336722.html},
  journal = {IEEE Software},
  keywords = {evolvability,Investments,maintainability,refactoring,Risk management,Software maintenance,software quality,Software quality,Sofware testing,technical debt},
  number = {6}
}

@misc{Kubeflow,
  title = {Kubeflow},
  abstract = {Kubeflow makes deployment of ML Workflows on Kubernetes straightforward and automated},
  file = {/Users/antonio/Zotero/storage/ETSFNT3K/www.kubeflow.org.html},
  howpublished = {https://www.kubeflow.org/},
  journal = {Kubeflow},
  language = {en}
}

@misc{Lab5bDatalore,
  title = {Lab5b ({{Datalore}})},
  file = {/Users/antonio/Zotero/storage/73U7RPMJ/MzzilwRMAz1lMVIc3dQwj8.html},
  howpublished = {https://datalore.jetbrains.com/notebook/Ga070iNACuddz9gv2iZWnm/MzzilwRMAz1lMVIc3dQwj8/}
}

@article{ladjalPCAlikeAutoencoder2019,
  title = {A {{PCA}}-like {{Autoencoder}}},
  author = {Ladjal, Sa{\"i}d and Newson, Alasdair and Pham, Chi-Hieu},
  year = {2019},
  month = apr,
  abstract = {An autoencoder is a neural network which data projects to and from a lower dimensional latent space, where this data is easier to understand and model. The autoencoder consists of two sub-networks, the encoder and the decoder, which carry out these transformations. The neural network is trained such that the output is as close to the input as possible, the data having gone through an information bottleneck : the latent space. This tool bears significant ressemblance to Principal Component Analysis (PCA), with two main differences. Firstly, the autoencoder is a non-linear transformation, contrary to PCA, which makes the autoencoder more flexible and powerful. Secondly, the axes found by a PCA are orthogonal, and are ordered in terms of the amount of variability which the data presents along these axes. This makes the interpretability of the PCA much greater than that of the autoencoder, which does not have these attributes. Ideally, then, we would like an autoencoder whose latent space consists of independent components, ordered by decreasing importance to the data. In this paper, we propose an algorithm to create such a network. We create an iterative algorithm which progressively increases the size of the latent space, learning a new dimension at each step. Secondly, we propose a covariance loss term to add to the standard autoencoder loss function, as well as a normalisation layer just before the latent space, which encourages the latent space components to be statistically independent. We demonstrate the results of this autoencoder on simple geometric shapes, and find that the algorithm indeed finds a meaningful representation in the latent space. This means that subsequent interpolation in the latent space has meaning with respect to the geometric properties of the images.},
  archivePrefix = {arXiv},
  eprint = {1904.01277},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/6R853WIW/Ladjal et al. - 2019 - A PCA-like Autoencoder.pdf;/Users/antonio/Zotero/storage/5ZCWLYD2/1904.html},
  journal = {arXiv:1904.01277 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{lawson1979basic,
  title = {Basic Linear Algebra Subprograms for {{Fortran}} Usage},
  author = {Lawson, Chuck L and Hanson, Richard J. and Kincaid, David R and Krogh, Fred T.},
  year = {1979},
  volume = {5},
  pages = {308--323},
  publisher = {{ACM New York, NY, USA}},
  journal = {ACM Transactions on Mathematical Software (TOMS)},
  number = {3}
}

@article{levenshteinBinaryCodesCapable1966,
  title = {Binary {{Codes Capable}} of {{Correcting Deletions}}, {{Insertions}} and {{Reversals}}},
  author = {Levenshtein, V. I.},
  year = {1966},
  month = feb,
  volume = {10},
  pages = {707},
  abstract = {Not Available},
  journal = {Soviet Physics Doklady}
}

@article{liHyperbandNovelBanditbased2017,
  title = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2017},
  month = jan,
  volume = {18},
  pages = {6765--6816},
  issn = {1532-4435},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  file = {/Users/antonio/Zotero/storage/K3EBPHUV/Li et al. - 2017 - Hyperband a novel bandit-based approach to hyperp.pdf},
  journal = {The Journal of Machine Learning Research},
  keywords = {deep learning,hyperparameter optimization,infinite-armed bandits,model selection,online optimization},
  number = {1}
}

@article{lubsandorzhievHistoryPhotomultiplierTube2006,
  title = {On the History of Photomultiplier Tube Invention},
  author = {Lubsandorzhiev, B. K.},
  year = {2006},
  month = nov,
  volume = {567},
  pages = {236--238},
  issn = {0168-9002},
  doi = {10.1016/j.nima.2006.05.221},
  abstract = {In this very short note we review some historical aspects of photomultiplier tube invention. It is our tribute to the memory of great Soviet-Russian physicist and engineer Leonid Aleksandrovitch Kubetsky whose life and scientific achievements are described briefly. Particular efforts are made to shed light on a controversial issue of who invented the first photomultiplier tube. It is asserted that if to recognize L.A. Kubetsky's priority on the photomultiplier tube invention the last Beaune Conference would be held on the eve of the 75th Anniversary of that great event.},
  file = {/Users/antonio/Zotero/storage/PEYJ7BCR/Lubsandorzhiev - 2006 - On the history of photomultiplier tube invention.pdf;/Users/antonio/Zotero/storage/IE57EP6C/S0168900206009260.html},
  journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  keywords = {Dynode,Photocathode,Photomultiplier,Secondary electrons},
  language = {en},
  number = {1},
  series = {Proceedings of the 4th {{International Conference}} on {{New Developments}} in {{Photodetection}}}
}

@article{luRecommenderSystems2012,
  title = {Recommender Systems},
  author = {L{\"u}, Linyuan and Medo, Mat{\'u}{\v s} and Yeung, Chi Ho and Zhang, Yi-Cheng and Zhang, Zi-Ke and Zhou, Tao},
  year = {2012},
  month = oct,
  volume = {519},
  pages = {1--49},
  issn = {0370-1573},
  doi = {10.1016/j.physrep.2012.02.006},
  abstract = {The ongoing rapid expansion of the Internet greatly increases the necessity of effective recommender systems for filtering the abundant information. Extensive research for recommender systems is conducted by a broad range of communities including social and computer scientists, physicists, and interdisciplinary researchers. Despite substantial theoretical and practical achievements, unification and comparison of different approaches are lacking, which impedes further advances. In this article, we review recent developments in recommender systems and discuss the major challenges. We compare and evaluate available algorithms and examine their roles in the future developments. In addition to algorithms, physical aspects are described to illustrate macroscopic behavior of recommender systems. Potential impacts and future directions are discussed. We emphasize that recommendation has great scientific depth and combines diverse research fields which makes it interesting for physicists as well as interdisciplinary researchers.},
  file = {/Users/antonio/Zotero/storage/H3I3BLFB/Lü et al. - 2012 - Recommender systems.pdf;/Users/antonio/Zotero/storage/FF74V8LU/S0370157312000828.html},
  journal = {Physics Reports},
  keywords = {Information filtering,Networks,Recommender systems},
  language = {en},
  number = {1},
  series = {Recommender {{Systems}}}
}

@misc{MachinableorgMachinable2020,
  title = {Machinable-Org/Machinable},
  year = {2020},
  month = jun,
  abstract = {A modular configuration system for machine learning research},
  copyright = {MIT},
  howpublished = {machinable},
  keywords = {convention-over-configuration,data-science,framework-agnostic,machine-learning,python-3}
}

@misc{MachineLearningTrick2015,
  title = {Machine {{Learning Trick}} of the {{Day}} (4): {{Reparameterisation Tricks}}},
  shorttitle = {Machine {{Learning Trick}} of the {{Day}} (4)},
  year = {2015},
  month = oct,
  abstract = {Our ability to rewrite statistical problems in an equivalent but different form, to reparameterise them, is one of the most general-purpose~tools we have~in mathematical statistics.~We used reparam\ldots},
  file = {/Users/antonio/Zotero/storage/C5IMUXVS/machine-learning-trick-of-the-day-4-reparameterisation-tricks.html},
  journal = {The Spectator},
  language = {en-GB}
}

@incollection{masonBoostingAlgorithmsGradient2000,
  title = {Boosting {{Algorithms}} as {{Gradient Descent}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 12},
  author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter L. and Frean, Marcus R.},
  editor = {Solla, S. A. and Leen, T. K. and M{\"u}ller, K.},
  year = {2000},
  pages = {512--518},
  publisher = {{MIT Press}},
  file = {/Users/antonio/Zotero/storage/SN589XP2/Mason et al. - 2000 - Boosting Algorithms as Gradient Descent.pdf;/Users/antonio/Zotero/storage/82GFUFKF/1766-boosting-algorithms-as-gradient-descent.html}
}

@inproceedings{mcilroy-youngAligningSuperhumanAI2020,
  title = {Aligning {{Superhuman AI}} with {{Human Behavior}}: {{Chess}} as a {{Model System}}},
  shorttitle = {Aligning {{Superhuman AI}} with {{Human Behavior}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {{McIlroy-Young}, Reid and Sen, Siddhartha and Kleinberg, Jon and Anderson, Ashton},
  year = {2020},
  month = aug,
  pages = {1677--1687},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3394486.3403219},
  abstract = {As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance. We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well. We develop and introduce Maia, a customized version of AlphaZero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making.},
  isbn = {978-1-4503-7998-4},
  keywords = {action prediction,chess,human-ai collaboration},
  series = {{{KDD}} '20}
}

@article{mcinnes2017hdbscan,
  title = {Hdbscan: {{Hierarchical}} Density Based Clustering},
  author = {McInnes, Leland and Healy, John and Astels, Steve},
  year = {2017},
  volume = {2},
  pages = {205},
  journal = {Journal of Open Source Software},
  number = {11}
}

@misc{mcknightDeliveringVisionMLOps2020,
  title = {Delivering on the {{Vision}} of {{MLOps}}},
  author = {McKnight, William},
  year = {2020},
  month = jan,
  publisher = {{Gigaom}},
  abstract = {This report is targeted at Business and IT decision-makers as they look to implement MLOps, which is an approach to deliver Machine\ldots},
  annotation = {Last Modified: 2020-06-11},
  file = {/Users/antonio/Zotero/storage/DYIRF5U3/delivering-on-the-vision-of-mlops.html},
  howpublished = {https://gigaom.com/report/delivering-on-the-vision-of-mlops/},
  language = {en-US}
}

@article{merkel2014docker,
  title = {Docker: Lightweight Linux Containers for Consistent Development and Deployment},
  author = {Merkel, Dirk},
  year = {2014},
  volume = {2014},
  pages = {2},
  journal = {Linux journal},
  number = {239}
}

@article{mesnardReproducibleWorkflowPublic2020,
  title = {Reproducible {{Workflow}} on a {{Public Cloud}} for {{Computational Fluid Dynamics}}},
  author = {Mesnard, Olivier and Barba, Lorena A.},
  year = {2020},
  month = jan,
  volume = {22},
  pages = {102--116},
  issn = {1521-9615, 1558-366X},
  doi = {10.1109/MCSE.2019.2941702},
  abstract = {In a new effort to make our research transparent and reproducible by others, we developed a workflow to run and share computational studies on the public cloud Microsoft Azure. It uses Docker containers to create an image of the application software stack. We also adopt several tools that facilitate creating and managing virtual machines on compute nodes and submitting jobs to these nodes. The configuration files for these tools are part of an expanded "reproducibility package" that includes workflow definitions for cloud computing, in addition to input files and instructions. This facilitates re-creating the cloud environment to re-run the computations under the same conditions. Although cloud providers have improved their offerings, many researchers using high-performance computing (HPC) are still skeptical about cloud computing. Thus, we ran benchmarks for tightly coupled applications to confirm that the latest HPC nodes of Microsoft Azure are indeed a viable alternative to traditional on-site HPC clusters. We also show that cloud offerings are now adequate to complete computational fluid dynamics studies with in-house research software that uses parallel computing with GPUs. Finally, we share with the community what we have learned from nearly two years of using Azure cloud to enhance transparency and reproducibility in our computational simulations.},
  archivePrefix = {arXiv},
  eprint = {1904.07981},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/2SH3FJE7/Mesnard and Barba - 2020 - Reproducible Workflow on a Public Cloud for Comput.pdf;/Users/antonio/Zotero/storage/38YZ5HE3/1904.html},
  journal = {Computing in Science \& Engineering},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Physics - Computational Physics},
  number = {1}
}

@book{millerLevenshteinDistanceInformation2009,
  title = {Levenshtein {{Distance}}: {{Information}} Theory, {{Computer}} Science, {{String}} (Computer Science), {{String}} Metric, {{Damerau}}?{{Levenshtein}} Distance, {{Spell}} Checker, {{Hamming}} Distance},
  shorttitle = {Levenshtein {{Distance}}},
  author = {Miller, Frederic P. and Vandome, Agnes F. and McBrewster, John},
  year = {2009},
  publisher = {{Alpha Press}},
  abstract = {In information theory and computer science, the Levenshtein distance is a metric for measuring the amount of difference between two sequences (i.e., the so called edit distance). The Levenshtein distance between two strings is given by the minimum number of operations needed to transform one string into the other, where an operation is an insertion, deletion, or substitution of a single character. A generalization of the Levenshtein distance (Damerau?Levenshtein distance) allows the transposition of two characters as an operation. Some Translation Environment Tools, such as translation memory leveraging applications, use the Levenhstein algorithm to measure the edit distance between two fuzzy matching content segments.The metric is named after Vladimir Levenshtein, who considered this distance in 1965. It is often used in applications that need to determine how similar, or different, two strings are, such as spell checkers},
  isbn = {978-613-0-21690-0}
}

@techreport{mitrouDataProtectionArtificial2018,
  title = {Data {{Protection}}, {{Artificial Intelligence}} and {{Cognitive Services}}: {{Is}} the {{General Data Protection Regulation}} ({{GDPR}}) `{{Artificial Intelligence}}-{{Proof}}'?},
  shorttitle = {Data {{Protection}}, {{Artificial Intelligence}} and {{Cognitive Services}}},
  author = {Mitrou, Lilian},
  year = {2018},
  month = dec,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3386914},
  abstract = {AI - in its interplay with Big Data, ambient intelligence, ubiquitous computing and cloud computing - augments the existing major, qualitative and quantitative, shift with regard to the processing of personal information. The questions that arise are of crucial importance both for the development of AI and the efficiency of data protection arsenal: Is the current legal framework AI-proof ? Are the data protection and privacy rules and principles adequate to deal with the challenges of AI or do we need to elaborate new principles to work alongside the advances of AI technology? Our research focuses on the assessment of GDPR that, however, does not specifically address AI, as the regulatory choice consisted more in what we perceive as ``technology \textendash{} independent legislation. The paper will give a critical overview and assessment of the provisions of GDPR that are relevant for the AI-environment, i.e. the scope of application, the legal grounds with emphasis on consent, the reach and applicability of  data protection principles and the new (accountability) tools to enhance and ensure compliance.},
  file = {/Users/antonio/Zotero/storage/FFH9V5JS/papers.html},
  keywords = {Artificial Intelligence,Ethics,GDPR,Privacy,Profiling,Transparency},
  language = {en},
  number = {ID 3386914},
  type = {{{SSRN Scholarly Paper}}}
}

@misc{MLOpsContinuousDelivery,
  title = {{{MLOps}}: {{Continuous}} Delivery and Automation Pipelines in Machine Learning},
  shorttitle = {{{MLOps}}},
  file = {/Users/antonio/Zotero/storage/SNZ6PI2V/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning.html},
  howpublished = {https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning},
  journal = {Google Cloud},
  language = {en}
}

@techreport{moraila2014measuring,
  title = {Measuring Reproducibility in Computer Systems Research},
  author = {Moraila, Gina and Shankaran, Akash and Shi, Zuoming and Warren, Alex M},
  year = {2014},
  institution = {{Technical report, University of Arizona}}
}

@article{moritzRayDistributedFramework2018,
  title = {Ray: {{A Distributed Framework}} for {{Emerging AI Applications}}},
  shorttitle = {Ray},
  author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
  year = {2018},
  month = sep,
  abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
  archivePrefix = {arXiv},
  eprint = {1712.05889},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/SR45P29N/Moritz et al. - 2018 - Ray A Distributed Framework for Emerging AI Appli.pdf;/Users/antonio/Zotero/storage/NY5Z2BWF/1712.html},
  journal = {arXiv:1712.05889 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{moritzRayDistributedFramework2018a,
  title = {Ray: {{A Distributed Framework}} for {{Emerging AI Applications}}},
  shorttitle = {Ray},
  author = {Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
  year = {2018},
  month = sep,
  abstract = {The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.},
  archivePrefix = {arXiv},
  eprint = {1712.05889},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/QZNTYXBE/Moritz et al. - 2018 - Ray A Distributed Framework for Emerging AI Appli.pdf;/Users/antonio/Zotero/storage/MLK9JBXC/1712.html},
  journal = {arXiv:1712.05889 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{MVIGSJTUAlphaPose2020,
  title = {{{MVIG}}-{{SJTU}}/{{AlphaPose}}},
  year = {2020},
  month = nov,
  abstract = {Real-Time and Accurate Full-Body Multi-Person Pose Estimation\&Tracking System},
  copyright = {View license         ,                 View license},
  howpublished = {Machine Vision and Intelligence Group @ SJTU},
  keywords = {accurate,alpha-pose,alphapose,crowdpose,full-body,gpu,human-pose-estimation,human-pose-tracking,human-tracking,person-pose-estimation,pose-estimation,posetracking,pytorch,realtime,state-of-the-art,torch,tracking,whole-body}
}

@article{nagarajanDeterministicImplementationsReproducibility2019,
  title = {Deterministic {{Implementations}} for {{Reproducibility}} in {{Deep Reinforcement Learning}}},
  author = {Nagarajan, Prabhat and Warnell, Garrett and Stone, Peter},
  year = {2019},
  month = jun,
  abstract = {While deep reinforcement learning (DRL) has led to numerous successes in recent years, reproducing these successes can be extremely challenging. One reproducibility challenge particularly relevant to DRL is nondeterminism in the training process, which can substantially affect the results. Motivated by this challenge, we study the positive impacts of deterministic implementations in eliminating nondeterminism in training. To do so, we consider the particular case of the deep Q-learning algorithm, for which we produce a deterministic implementation by identifying and controlling all sources of nondeterminism in the training process. One by one, we then allow individual sources of nondeterminism to affect our otherwise deterministic implementation, and measure the impact of each source on the variance in performance. We find that individual sources of nondeterminism can substantially impact the performance of agent, illustrating the benefits of deterministic implementations. In addition, we also discuss the important role of deterministic implementations in achieving exact replicability of results.},
  archivePrefix = {arXiv},
  eprint = {1809.05676},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/X8CJNQFV/Nagarajan et al. - 2019 - Deterministic Implementations for Reproducibility .pdf;/Users/antonio/Zotero/storage/QYT632QA/1809.html},
  journal = {arXiv:1809.05676 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@article{naglerSustainabilityReproducibilityContainerized2015,
  title = {Sustainability and {{Reproducibility}} via {{Containerized Computing}}},
  author = {Nagler, Robert and Bruhwiler, David and Moeller, Paul and Webb, Stephen},
  year = {2015},
  month = sep,
  abstract = {Recent developments in the commercial open source community have catalysed the use of Linux containers for scalable deployment of web-based applications to the cloud. Scientific software can be containerized with dependencies, configuration files, post-processing tools and even simulation results, referred to as containerized computing. This new approach promises to significantly improve sustainability, productivity and reproducibility. We present our experiences, technology, and future plans for open source containerization of software used to model particle and radiation beams. Vagrant is central to our approach, using Docker for cloud deployment and VirtualBox virtual machines for deployment to Mac OS and Windows computers. Our technology enables seamless switching between the desktop and the cloud to simplify simulation development and execution.},
  archivePrefix = {arXiv},
  eprint = {1509.08789},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/QLPXNETD/Nagler et al. - 2015 - Sustainability and Reproducibility via Containeriz.pdf;/Users/antonio/Zotero/storage/7F2BW8LH/1509.html},
  journal = {arXiv:1509.08789 [cs]},
  keywords = {Computer Science - Software Engineering,D.2.12,D.2.m,K.6.1,K.6.3},
  primaryClass = {cs}
}

@article{nakandalaTamingModelServing,
  title = {Taming {{Model Serving Complexity}}, {{Performance}} and {{Cost}}: {{A Compilation}} to {{Tensor Computations Approach}}},
  author = {Nakandala, Supun and Saur, Karla and Yu, Gyeong-In and Karanasos, Konstantinos and Curino, Carlo and Weimer, Markus and Interlandi, Matteo},
  pages = {16},
  abstract = {Machine Learning (ML) adoption in the enterprise requires simpler and more efficient software infrastructure\textemdash the bespoke solutions typical in large web companies are simply untenable. Model scoring, the process of obtaining prediction from a trained model over new data, is a primary contributor to infrastructure complexity and cost, as models are trained once but used many times.},
  file = {/Users/antonio/Zotero/storage/SDCJ2KAS/Nakandala et al. - Taming Model Serving Complexity, Performance and C.pdf},
  language = {en}
}

@article{newcombeIntervalEstimationDifference1998,
  title = {Interval Estimation for the Difference between Independent Proportions: Comparison of Eleven Methods},
  shorttitle = {Interval Estimation for the Difference between Independent Proportions},
  author = {Newcombe, Robert G.},
  year = {1998},
  volume = {17},
  pages = {873--890},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19980430)17:8<873::AID-SIM779>3.0.CO;2-I},
  abstract = {Several existing unconditional methods for setting confidence intervals for the difference between binomial proportions are evaluated. Computationally simpler methods are prone to a variety of aberrations and poor coverage properties. The closely interrelated methods of Mee and Miettinen and Nurminen perform well but require a computer program. Two new approaches which also avoid aberrations are developed and evaluated. A tail area profile likelihood based method produces the best coverage properties, but is difficult to calculate for large denominators. A method combining Wilson score intervals for the two proportions to be compared also performs well, and is readily implemented irrespective of sample size. \textcopyright{} 1998 John Wiley \& Sons, Ltd.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-0258\%2819980430\%2917\%3A8\%3C873\%3A\%3AAID-SIM779\%3E3.0.CO\%3B2-I},
  copyright = {Copyright \textcopyright{} 1998 John Wiley \& Sons, Ltd.},
  file = {/Users/antonio/Zotero/storage/JI7AXKRK/(SICI)1097-0258(19980430)178873AID-SIM7793.0.html},
  journal = {Statistics in Medicine},
  language = {en},
  number = {8}
}

@article{ng2011sparse,
  title = {Sparse Autoencoder},
  author = {Ng, Andrew and others},
  year = {2011},
  volume = {72},
  pages = {1--19},
  journal = {CS294A Lecture notes},
  number = {2011}
}

@misc{NumPy,
  title = {{{NumPy}}},
  file = {/Users/antonio/Zotero/storage/6PU3RLAV/numpy.org.html},
  howpublished = {https://numpy.org/}
}

@article{nwankpaActivationFunctionsComparison2018,
  title = {Activation {{Functions}}: {{Comparison}} of Trends in {{Practice}} and {{Research}} for {{Deep Learning}}},
  shorttitle = {Activation {{Functions}}},
  author = {Nwankpa, Chigozie and Ijomah, Winifred and Gachagan, Anthony and Marshall, Stephen},
  year = {2018},
  month = nov,
  abstract = {Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date.},
  archivePrefix = {arXiv},
  eprint = {1811.03378},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/4MRZJBXS/Nwankpa et al. - 2018 - Activation Functions Comparison of trends in Prac.pdf;/Users/antonio/Zotero/storage/PQISDQP6/1811.html},
  journal = {arXiv:1811.03378 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{olorisadeReproducibilityMachineLearningBased2017,
  title = {Reproducibility in {{Machine Learning}}-{{Based Studies}}: {{An Example}} of {{Text Mining}}},
  shorttitle = {Reproducibility in {{Machine Learning}}-{{Based Studies}}},
  booktitle = {Reproducibility in {{ML Workshop}} at the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Olorisade, Babatunde Kazeem and Brereton, Pearl and Andras, Peter},
  year = {2017},
  publisher = {{ICML}},
  abstract = {Reproducibility is an essential requirement for computational studies including those based on machine learning techniques. However, many machine learning studies are either not reproducible or are difficult to reproduce. In this paper, we consider what information about text mining studies is crucial to successful reproduction of such studies. We identify a set of factors that affect reproducibility based on our experience of attempting to reproduce six studies proposing text mining techniques for the automation of the citation screening stage in the systematic review process. Subsequently, the reproducibility of 30 studies was evaluated based on the presence or otherwise of information relating to the factors. While the studies provide useful reports of their results, they lack information on access to the dataset in the form and order as used in the original study (as against raw data), the software environment used, randomization control and the implementation of proposed techniques. In order to increase the chances of being reproduced, researchers should ensure that details about and/or access to information about these factors are provided in their reports.},
  file = {/Users/antonio/Zotero/storage/WLAJVE64/Olorisade et al. - 2017 - Reproducibility in Machine Learning-Based Studies.pdf}
}

@article{olorisadeReproducibilityMachineLearningBased2017a,
  title = {Reproducibility in {{Machine Learning}}-{{Based Studies}}: {{An Example}} of {{Text Mining}}},
  shorttitle = {Reproducibility in {{Machine Learning}}-{{Based Studies}}},
  author = {Olorisade, Babatunde K. and Brereton, Pearl and Andras, Peter},
  year = {2017},
  month = jun,
  abstract = {Reproducibility is an essential requirement for computational studies including those based on machine learning techniques. However, many machine learning studies are either not reproducible or are...},
  file = {/Users/antonio/Zotero/storage/G34I485B/Olorisade et al. - 2017 - Reproducibility in Machine Learning-Based Studies.pdf;/Users/antonio/Zotero/storage/SZSDT458/forum.html}
}

@misc{OnnxOnnx2020,
  title = {Onnx/Onnx},
  year = {2020},
  month = jun,
  abstract = {Open standard for machine learning interoperability},
  copyright = {MIT},
  howpublished = {Open Neural Network Exchange},
  keywords = {deep-learning,deep-neural-networks,dnn,keras,machine-learning,ml,mxnet,neural-network,onnx,pytorch,scikit-learn,tensorflow}
}

@article{oordConditionalImageGeneration2016,
  title = {Conditional {{Image Generation}} with {{PixelCNN Decoders}}},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  archivePrefix = {arXiv},
  eprint = {1606.05328},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/ZJKHZHKK/Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoder.pdf;/Users/antonio/Zotero/storage/CFTENWRB/1606.html},
  journal = {arXiv:1606.05328 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{oordPixelRecurrentNeural2016,
  title = {Pixel {{Recurrent Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Oord, Aaron Van and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  pages = {1747--1756},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep n...},
  file = {/Users/antonio/Zotero/storage/9DG2IUG7/Oord et al. - 2016 - Pixel Recurrent Neural Networks.pdf;/Users/antonio/Zotero/storage/7RGIV56W/oord16.html},
  language = {en}
}

@article{oordWaveNetGenerativeModel2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archivePrefix = {arXiv},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/SUUJZTG8/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf;/Users/antonio/Zotero/storage/AJZ2IUZW/1609.html},
  journal = {arXiv:1609.03499 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  primaryClass = {cs}
}

@article{ostapchenkoQGSJETIIReliableDescription2006,
  title = {{{QGSJET}}-{{II}}: Towards Reliable Description of Very High Energy Hadronic Interactions},
  shorttitle = {{{QGSJET}}-{{II}}},
  author = {Ostapchenko, S.},
  year = {2006},
  month = jan,
  volume = {151},
  pages = {143--146},
  issn = {0920-5632},
  doi = {10.1016/j.nuclphysbps.2005.07.026},
  abstract = {Since a number of years the QGSJET model has been successfully used by different groups in the field of high energy cosmic rays. Current work is devoted to the first general update of the model. The key improvement is connected to an account for non-linear interaction effects which are of crucial importance for reliable model extrapolation into the ultra-high energy domain. The proposed formalism allows to obtain a consistent description of hadron-hadron cross sections and hadron structure functions and to treat non-linear effects explicitly in individual hadronic and nuclear collisions. Other ameliorations concern the treatment of low mass diffraction, employment of realistic nuclear density profiles, and re-calibration of model parameters using a wider set of accelerator data.},
  file = {/Users/antonio/Zotero/storage/CX9SAPLG/Ostapchenko - 2006 - QGSJET-II towards reliable description of very hi.pdf;/Users/antonio/Zotero/storage/4CI5U37A/S0920563205009175.html},
  journal = {Nuclear Physics B - Proceedings Supplements},
  language = {en},
  number = {1},
  series = {{{VERY HIGH ENERGY COSMIC RAY INTERACTIONS}}}
}

@article{pengReproducibilityCrisisScience2015,
  title = {The Reproducibility Crisis in Science: {{A}} Statistical Counterattack},
  shorttitle = {The Reproducibility Crisis in Science},
  author = {Peng, Roger},
  year = {2015},
  volume = {12},
  pages = {30--32},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2015.00827.x},
  abstract = {More people have more access to data than ever before. But a comparative lack of analytical skills has resulted in scientific findings that are neither replicable nor reproducible. It is time to invest in statistics education, says Roger Peng},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2015.00827.x},
  copyright = {\textcopyright{} 2015 The Royal Statistical Society},
  file = {/Users/antonio/Zotero/storage/LUEISVHW/Peng - 2015 - The reproducibility crisis in science A statistic.pdf;/Users/antonio/Zotero/storage/JATDMW47/j.1740-9713.2015.00827.html},
  journal = {Significance},
  language = {en},
  number = {3}
}

@article{pengReproducibleResearchComputational2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  year = {2011},
  month = dec,
  volume = {334},
  pages = {1226--1227},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  chapter = {Perspective},
  copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
  file = {/Users/antonio/Zotero/storage/JNS42KDW/Peng - 2011 - Reproducible Research in Computational Science.pdf;/Users/antonio/Zotero/storage/XRAKPYDQ/tab-pdf.html},
  journal = {Science},
  language = {en},
  number = {6060},
  pmid = {22144613}
}

@article{pierogEPOSLHCTest2015,
  title = {{{EPOS LHC}} : Test of Collective Hadronization with {{LHC}} Data},
  shorttitle = {{{EPOS LHC}}},
  author = {Pierog, T. and Karpenko, Iu and Katzy, J. M. and Yatsenko, E. and Werner, K.},
  year = {2015},
  month = sep,
  volume = {92},
  pages = {034906},
  issn = {0556-2813, 1089-490X},
  doi = {10.1103/PhysRevC.92.034906},
  abstract = {EPOS is a Monte-Carlo event generator for minimum bias hadronic interactions, used for both heavy ion interactions and cosmic ray air shower simulations. Since the last public release in 2009, the LHC experiments have provided a number of very interesting data sets comprising minimum bias p-p, p-Pb and Pb-Pb interactions. We describe the changes required to the model to reproduce in detail the new data available from LHC and the consequences in the interpretation of these data. In particular we discuss the effect of the collective hadronization in p-p scattering. A different parametrization of flow has been introduced in the case of a small volume with high density of thermalized matter (core) reached in p-p compared to large volume produced in heavy ion collisions. Both parametrizations depend only on the geometry and the amount of secondary particles entering in the core and not on the beam mass or energy. The transition between the two flow regimes can be tested with p-Pb data. EPOS LHC is able to reproduce all minimum bias},
  archivePrefix = {arXiv},
  eprint = {1306.0121},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/TMKZYQPF/Pierog et al. - 2015 - EPOS LHC  test of collective hadronization with L.pdf;/Users/antonio/Zotero/storage/ILKILCPH/1306.html},
  journal = {Physical Review C},
  keywords = {High Energy Physics - Phenomenology},
  number = {3}
}

@article{PierreAugerCosmic2015,
  title = {The {{Pierre Auger Cosmic Ray Observatory}}},
  year = {2015},
  month = oct,
  volume = {798},
  pages = {172--213},
  issn = {0168-9002},
  doi = {10.1016/j.nima.2015.06.058},
  abstract = {The Pierre Auger Observatory, located on a vast, high plain in western Argentina, is the world׳s largest cosmic ray observatory. The objectives of the Observatory are to probe the origin and characteristics of cosmic rays above 1017eV and to study the interactions of these, the most energetic particles observed in nature. The Auger design features an array of 1660 water Cherenkov particle detector stations spread over 3000km2 overlooked by 24 air fluorescence telescopes. In addition, three high elevation fluorescence telescopes overlook a 23.5km2, 61-detector infilled array with 750m spacing. The Observatory has been in successful operation since completion in 2008 and has recorded data from an exposure exceeding 40,000km2sryr. This paper describes the design and performance of the detectors, related subsystems and infrastructure that make up the Observatory.},
  file = {/Users/antonio/Zotero/storage/GU7NXNIM/2015 - The Pierre Auger Cosmic Ray Observatory.pdf;/Users/antonio/Zotero/storage/8Q99IG33/S0168900215008086.html},
  journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  keywords = {Air fluorescence detectors,High energy cosmic rays,Hybrid observatory,Pierre Auger Observatory,Water Cherenkov detectors},
  language = {en}
}

@misc{PolyaxonMachineLearning,
  title = {Polyaxon - Machine Learning at Scale},
  abstract = {Get familiar with Polyaxon - Open source machine learning on Kubernetes, deep Learning on Kubernetes.},
  file = {/Users/antonio/Zotero/storage/GTHAPTCK/polyaxon.com.html},
  howpublished = {https://polyaxon.com/},
  journal = {Polyaxon},
  language = {en}
}

@misc{PolyaxonPolyaxon2020,
  title = {Polyaxon/Polyaxon},
  year = {2020},
  month = jun,
  abstract = {Cloud native machine learning automation platform. Contribute to polyaxon/polyaxon development by creating an account on GitHub.},
  copyright = {Apache-2.0},
  howpublished = {polyaxon},
  keywords = {ai,artificial-intelligence,caffe,data-science,deep-learning,jupyter,jupyterlab,k8s,keras,kubernetes,machine-learning,ml,mxnet,notebook,pipelines,polyaxon,pytorch,reinforcement-learning,tensorflow,workflow}
}

@article{polyzotisDataLifecycleChallenges2018,
  title = {Data {{Lifecycle Challenges}} in {{Production Machine Learning}}: {{A Survey}}},
  shorttitle = {Data {{Lifecycle Challenges}} in {{Production Machine Learning}}},
  author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  year = {2018},
  month = dec,
  volume = {47},
  pages = {17--28},
  issn = {0163-5808},
  doi = {10.1145/3299887.3299891},
  abstract = {Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.},
  file = {/Users/antonio/Zotero/storage/I95ASSUJ/Polyzotis et al. - 2018 - Data Lifecycle Challenges in Production Machine Le.pdf},
  journal = {ACM SIGMOD Record},
  number = {2}
}

@inproceedings{polyzotisDataManagementChallenges2017,
  title = {Data {{Management Challenges}} in {{Production Machine Learning}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Polyzotis, Neoklis and Roy, Sudip and Whang, Steven Euijong and Zinkevich, Martin},
  year = {2017},
  month = may,
  pages = {1723--1726},
  publisher = {{Association for Computing Machinery}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/3035918.3054782},
  abstract = {The tutorial discusses data-management issues that arise in the context of machine learning pipelines deployed in production. Informed by our own experience with such largescale pipelines, we focus on issues related to understanding, validating, cleaning, and enriching training data. The goal of the tutorial is to bring forth these issues, draw connections to prior work in the database literature, and outline the open research questions that are not addressed by prior art.},
  file = {/Users/antonio/Zotero/storage/ZX633K3H/Polyzotis et al. - 2017 - Data Management Challenges in Production Machine L.pdf},
  isbn = {978-1-4503-4197-4},
  keywords = {data enrichment,data management,data understanding,data validation,machine learning,production},
  series = {{{SIGMOD}} '17}
}

@article{poppComprehensiveSupportLifecycle,
  title = {Comprehensive {{Support}} of the {{Lifecycle}} of {{Machine Learning Models}} in {{Model Management Systems}}},
  author = {Popp, Matthias},
  pages = {71},
  file = {/Users/antonio/Zotero/storage/99DYX5WW/Popp - Comprehensive Support of the Lifecycle of Machine .pdf},
  language = {en}
}

@misc{ProductionGradeContainerOrchestration,
  title = {Production-{{Grade Container Orchestration}}},
  abstract = {Production-Grade Container Orchestration},
  file = {/Users/antonio/Zotero/storage/M9DDH9UE/kubernetes.io.html},
  howpublished = {https://kubernetes.io/},
  journal = {Kubernetes},
  language = {en}
}

@misc{ProjectJupyter,
  title = {Project {{Jupyter}}},
  abstract = {The Jupyter Notebook is a web-based interactive computing platform. The notebook combines live code, equations, narrative text, visualizations, interactive dashboards and other media.},
  file = {/Users/antonio/Zotero/storage/HYC9HASR/jupyter.org.html},
  howpublished = {https://www.jupyter.org}
}

@article{provost1998glossary,
  title = {Glossary of Terms},
  author = {Provost, Foster and Kohavi, R},
  year = {1998},
  volume = {30},
  pages = {271--274},
  journal = {Journal of Machine Learning},
  number = {2-3}
}

@article{raffStepQuantifyingIndependently2019,
  title = {A {{Step Toward Quantifying Independently Reproducible Machine Learning Research}}},
  author = {Raff, Edward},
  year = {2019},
  month = sep,
  abstract = {What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.},
  archivePrefix = {arXiv},
  eprint = {1909.06674},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/F2I4GYTS/Raff - 2019 - A Step Toward Quantifying Independently Reproducib.pdf;/Users/antonio/Zotero/storage/9BX6KXUE/1909.html},
  journal = {arXiv:1909.06674 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Digital Libraries,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ramachandranSearchingActivationFunctions2017,
  title = {Searching for {{Activation Functions}}},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  year = {2017},
  month = oct,
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x \textbackslash cdot \textbackslash text\{sigmoid\}(\textbackslash beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\textbackslash\% for Mobile NASNet-A and 0.6\textbackslash\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  archivePrefix = {arXiv},
  eprint = {1710.05941},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/A4V6553F/Ramachandran et al. - 2017 - Searching for Activation Functions.pdf;/Users/antonio/Zotero/storage/VZHVHZM5/1710.html},
  journal = {arXiv:1710.05941 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{rampinReproZipReproducibilityPacker2016,
  title = {{{ReproZip}}: {{The Reproducibility Packer}}},
  shorttitle = {{{ReproZip}}},
  author = {Rampin, R{\'e}mi and Chirigati, Fernando and Shasha, Dennis and Freire, Juliana and Steeves, Vicky},
  year = {2016},
  month = dec,
  volume = {1},
  pages = {107},
  issn = {2475-9066},
  doi = {10.21105/joss.00107},
  abstract = {Rampin et al, (2016), ReproZip: The Reproducibility Packer, Journal of Open Source Software, 1(8), 107, doi:10.21105/joss.00107},
  file = {/Users/antonio/Zotero/storage/TPIGJHGZ/Rampin et al. - 2016 - ReproZip The Reproducibility Packer.pdf;/Users/antonio/Zotero/storage/K6YRLUHJ/joss.html},
  journal = {Journal of Open Source Software},
  language = {en},
  number = {8}
}

@misc{reitermanovaDataSplitting2010,
  title = {Data {{Splitting}}},
  author = {Reitermanov{\'a}, Z.},
  year = {2010},
  abstract = {In machine learning, one of the main requirements is to build computational models with a high ability to generalize well the extracted knowledge. When training e.g. artificial neural networks, poor generalization is often characterized by over-training. A common method to avoid over-training is the hold-out crossvalidation. The basic problem of this method represents, however, appropriate data splitting. In most of the applications, simple random sampling is used. Nevertheless, there are several sophisticated statistical sampling methods suitable for various types of datasets. This paper provides a survey of existing sampling methods applicable to the data splitting problem. Supporting experiments evaluating the benefits of the selected data splitting techniques involve artificial neural networks of the back-propagation type.},
  file = {/Users/antonio/Zotero/storage/H73LAE7I/3804cb787031aacd41c2de320bc4ddad637238a3.html},
  howpublished = {/paper/Data-Splitting-Reitermanov\%C3\%A1/3804cb787031aacd41c2de320bc4ddad637238a3},
  language = {en}
}

@misc{reitermanovaDataSplitting2010a,
  title = {Data {{Splitting}}},
  author = {Reitermanov{\'a}, Z.},
  year = {2010},
  abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
  file = {/Users/antonio/Zotero/storage/XGCAFVC8/52b7bf3ba59b31f362aa07f957f1543a29a4279e.html},
  howpublished = {/paper/Support-Vector-Networks-Cortes-Vapnik/52b7bf3ba59b31f362aa07f957f1543a29a4279e},
  language = {en}
}

@article{reOvertonDataSystem2019,
  title = {Overton: {{A Data System}} for {{Monitoring}} and {{Improving Machine}}-{{Learned Products}}},
  shorttitle = {Overton},
  author = {R{\'e}, Christopher and Niu, Feng and Gudipati, Pallavi and Srisuwananukorn, Charles},
  year = {2019},
  month = sep,
  abstract = {We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production machine learning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton's vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks. In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year, Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing. In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors 1.7-2.9 times versus production systems.},
  archivePrefix = {arXiv},
  eprint = {1909.05372},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/KPW9VMYS/Ré et al. - 2019 - Overton A Data System for Monitoring and Improvin.pdf;/Users/antonio/Zotero/storage/RPVMKU8J/1909.html},
  journal = {arXiv:1909.05372 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{rohSurveyDataCollection2019,
  title = {A {{Survey}} on {{Data Collection}} for {{Machine Learning}}: A {{Big Data}} -- {{AI Integration Perspective}}},
  shorttitle = {A {{Survey}} on {{Data Collection}} for {{Machine Learning}}},
  author = {Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
  year = {2019},
  month = aug,
  abstract = {Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.},
  archivePrefix = {arXiv},
  eprint = {1811.03402},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/LMAZN8KW/Roh et al. - 2019 - A Survey on Data Collection for Machine Learning .pdf;/Users/antonio/Zotero/storage/E4TVF46K/1811.html},
  journal = {arXiv:1811.03402 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{ruizendaalDeepLearningWhy2020,
  title = {Deep {{Learning}} \#4: {{Why You Need}} to {{Start Using Embedding Layers}}},
  shorttitle = {Deep {{Learning}} \#4},
  author = {Ruizendaal, Rutger},
  year = {2020},
  month = jun,
  abstract = {And how there's more to it than word embeddings.},
  file = {/Users/antonio/Zotero/storage/WCMPDJ3F/deep-learning-4-embedding-layers-f9a02d55ac12.html},
  howpublished = {https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12},
  journal = {Medium},
  language = {en}
}

@article{salimansPixelCNNImprovingPixelCNN2017,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with {{Discretized Logistic Mixture Likelihood}} and {{Other Modifications}}},
  shorttitle = {{{PixelCNN}}++},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  year = {2017},
  month = jan,
  abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  archivePrefix = {arXiv},
  eprint = {1701.05517},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/TIRNUSXV/Salimans et al. - 2017 - PixelCNN++ Improving the PixelCNN with Discretize.pdf;/Users/antonio/Zotero/storage/REJBR32F/1701.html},
  journal = {arXiv:1701.05517 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{sandveTenSimpleRules2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  year = {2013},
  month = oct,
  volume = {9},
  pages = {e1003285},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003285},
  file = {/Users/antonio/Zotero/storage/P69DUYQW/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf;/Users/antonio/Zotero/storage/GP3TQXSI/article.html},
  journal = {PLOS Computational Biology},
  keywords = {Archives,Computer and information sciences,Computer applications,Habits,Replication studies,Reproducibility,Sequence analysis,Source code},
  language = {en},
  number = {10}
}

@book{sartorImpactGeneralData2020,
  title = {The Impact of the {{General Data Protection Regulation}} ({{GDPR}}) on Artificial Intelligence: Study},
  shorttitle = {The Impact of the {{General Data Protection Regulation}} ({{GDPR}}) on Artificial Intelligence},
  author = {Sartor, Giovanni and {European Parliament} and {European Parliamentary Research Service} and {Scientific Foresight Unit}},
  year = {2020},
  abstract = {This study addresses the relation between the EU General Data Protection Regulation (GDPR) and artificial intelligence (AI). It considers challenges and opportunities for individuals and society, and the ways in which risks can be countered and opportunities enabled through law and technology. The study discusses the tensions and proximities between AI and data protection principles, such as in particular purpose limitation and data minimisation. It makes a thorough analysis of automated decision-making, considering the extent to which it is admissible, the safeguard measures to be adopted, and whether data subjects have a right to individual explanations. The study then considers the extent to which the GDPR provides for a preventive risk-based approach, focused on data protection by design and by default.},
  annotation = {OCLC: 1160193938},
  file = {/Users/antonio/Zotero/storage/5XSPYEU9/Sartor et al. - 2020 - The impact of the General Data Protection Regulati.pdf},
  isbn = {978-92-846-6771-0},
  language = {en}
}

@book{sartorImpactGeneralData2020a,
  title = {The Impact of the {{General Data Protection Regulation}} ({{GDPR}}) on Artificial Intelligence: Study},
  shorttitle = {The Impact of the {{General Data Protection Regulation}} ({{GDPR}}) on Artificial Intelligence},
  author = {Sartor, Giovanni and {European Parliament} and {European Parliamentary Research Service} and {Scientific Foresight Unit}},
  year = {2020},
  abstract = {This study addresses the relation between the EU General Data Protection Regulation (GDPR) and artificial intelligence (AI). It considers challenges and opportunities for individuals and society, and the ways in which risks can be countered and opportunities enabled through law and technology. The study discusses the tensions and proximities between AI and data protection principles, such as in particular purpose limitation and data minimisation. It makes a thorough analysis of automated decision-making, considering the extent to which it is admissible, the safeguard measures to be adopted, and whether data subjects have a right to individual explanations. The study then considers the extent to which the GDPR provides for a preventive risk-based approach, focused on data protection by design and by default.},
  annotation = {OCLC: 1160193938},
  file = {/Users/antonio/Zotero/storage/3ZUCW48Y/Sartor et al. - 2020 - The impact of the General Data Protection Regulati.pdf},
  isbn = {978-92-846-6771-0},
  language = {en}
}

@inproceedings{Schapire2013ExplainingA,
  title = {Explaining {{AdaBoost}}},
  booktitle = {Empirical Inference},
  author = {Schapire, R.},
  year = {2013}
}

@article{schelterChallengesMachineLearning2018,
  title = {On Challenges in Machine Learning Model Management.},
  author = {Schelter, Sebastian and Biessmann, Felix and Januschowski, Tim and Salinas, David and Seufert, Stephan and Szarvas, Gyuri and Deshpande, A},
  year = {2018},
  volume = {41},
  pages = {5--15},
  journal = {IEEE Data Eng. Bull.},
  number = {4}
}

@article{schoberCorrelationCoefficientsAppropriate2018,
  title = {Correlation {{Coefficients}}: {{Appropriate Use}} and {{Interpretation}}},
  shorttitle = {Correlation {{Coefficients}}},
  author = {Schober, Patrick and Boer, Christa and Schwarte, Lothar A.},
  year = {2018},
  month = may,
  volume = {126},
  pages = {1763--1768},
  doi = {10.1213/ANE.0000000000002864},
  abstract = {Correlation in the broadest sense is a measure of an association between variables. In correlated data, the change in the magnitude of 1 variable is associated with a change in the magnitude of another variable, either in the same (positive correlation) or in the opposite (negative correlation) direction. Most often, the term correlation is used in the context of a linear relationship between 2 continuous variables and expressed as Pearson product-moment correlation. The Pearson correlation coefficient is typically used for jointly normally distributed data (data that follow a bivariate normal distribution). For nonnormally distributed continuous data, for ordinal data, or for data with relevant outliers, a Spearman rank correlation can be used as a measure of a monotonic association. Both correlation coefficients are scaled such that they range from \textendash 1 to +1, where 0 indicates that there is no linear or monotonic association, and the relationship gets stronger and ultimately approaches a straight line (Pearson correlation) or a constantly increasing or decreasing curve (Spearman correlation) as the coefficient approaches an absolute value of 1. Hypothesis tests and confidence intervals can be used to address the statistical significance of the results and to estimate the strength of the relationship in the population from which the data were sampled. The aim of this tutorial is to guide researchers and clinicians in the appropriate use and interpretation of correlation coefficients.},
  journal = {Anesthesia \& Analgesia},
  number = {5}
}

@inproceedings{sculleyHiddenTechnicalDebt2015,
  title = {Hidden Technical Debt in {{Machine}} Learning Systems},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
  year = {2015},
  month = dec,
  pages = {2503--2511},
  publisher = {{MIT Press}},
  address = {{Montreal, Canada}},
  abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
  series = {{{NIPS}}'15}
}

@inproceedings{sculleyMachineLearningHigh2014,
  title = {Machine {{Learning}}: {{The High Interest Credit Card}} of {{Technical Debt}}},
  shorttitle = {Machine {{Learning}}},
  booktitle = {{{SE4ML}}: {{Software Engineering}} for {{Machine Learning}} ({{NIPS}} 2014 {{Workshop}})},
  author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
  year = {2014},
  file = {/Users/antonio/Zotero/storage/FNFY4ATS/Sculley et al. - 2014 - Machine Learning The High Interest Credit Card of.pdf}
}

@misc{seitaLearningLearn,
  title = {Learning to {{Learn}}},
  author = {Seita, Daniel},
  abstract = {The BAIR Blog},
  file = {/Users/antonio/Zotero/storage/2GHFT4RJ/learning-to-learn.html},
  howpublished = {http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/},
  journal = {The Berkeley Artificial Intelligence Research Blog}
}

@article{sharma2017activation,
  title = {Activation Functions in Neural Networks},
  author = {Sharma, Sagar},
  year = {2017},
  volume = {6},
  journal = {Towards Data Science}
}

@article{shawahnaFPGABasedAcceleratorsDeep2019,
  title = {{{FPGA}}-{{Based Accelerators}} of {{Deep Learning Networks}} for {{Learning}} and {{Classification}}: {{A Review}}},
  shorttitle = {{{FPGA}}-{{Based Accelerators}} of {{Deep Learning Networks}} for {{Learning}} and {{Classification}}},
  author = {Shawahna, Ahmad and Sait, Sadiq M. and {El-Maleh}, Aiman},
  year = {2019},
  volume = {7},
  pages = {7823--7859},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2890150},
  abstract = {Due to recent advances in digital technologies, and availability of credible data, an area of artificial intelligence, deep learning, has emerged and has demonstrated its ability and effectiveness in solving complex learning problems not possible before. In particular, convolutional neural networks (CNNs) have demonstrated their effectiveness in the image detection and recognition applications. However, they require intensive CPU operations and memory bandwidth that make general CPUs fail to achieve the desired performance levels. Consequently, hardware accelerators that use application-specific integrated circuits, field-programmable gate arrays (FPGAs), and graphic processing units have been employed to improve the throughput of CNNs. More precisely, FPGAs have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism and their energy efficiency. In this paper, we review the recent existing techniques for accelerating deep learning networks on FPGAs. We highlight the key features employed by the various techniques for improving the acceleration performance. In addition, we provide recommendations for enhancing the utilization of FPGAs for CNNs acceleration. The techniques investigated in this paper represent the recent trends in the FPGA-based accelerators of deep learning networks. Thus, this paper is expected to direct the future advances on efficient hardware accelerators and to be useful for deep learning researchers.},
  file = {/Users/antonio/Zotero/storage/2PNIP82A/Shawahna et al. - 2019 - FPGA-Based Accelerators of Deep Learning Networks .pdf;/Users/antonio/Zotero/storage/P447XT82/8594633.html},
  journal = {IEEE Access},
  keywords = {Acceleration,Adaptable architectures,Convolution,convolutional neural networks (CNNs),deep learning,Deep learning,dynamic reconfiguration,energy-efficient architecture,Field programmable gate arrays,field programmable gate arrays (FPGAs),Hardware,hardware accelerator,machine learning,neural networks,Neural networks,optimization,parallel computer architecture,reconfigurable computing,Throughput}
}

@book{shiPhotodiodesCommunicationsBioSensings2011,
  title = {Photodiodes: {{Communications}}, {{Bio}}-{{Sensings}}, {{Measurements}} and {{High}}-{{Energy Physics}}},
  shorttitle = {Photodiodes},
  author = {Shi, Jin-Wei},
  year = {2011},
  month = sep,
  publisher = {{BoD \textendash{} Books on Demand}},
  abstract = {This book describes different kinds of photodiodes for applications in high-speed data communication, biomedical sensing, high-speed measurement, UV-light detection, and high energy physics. The photodiodes discussed are composed of several different semiconductor materials, such as InP, SiC, and Si, which cover an extremely wide optical wavelength regime ranging from infrared light to X-ray, making the suitable for diversified applications. Several interesting and unique topics were discussed including: the operation of high-speed photodiodes at low-temperature for super-conducting electronics, photodiodes for bio-medical imaging, single photon detection, photodiodes for the applications in nuclear physics, and for UV-light detection.},
  googlebooks = {lNqgDwAAQBAJ},
  isbn = {978-953-307-277-7},
  keywords = {Technology \& Engineering / Electrical,Technology \& Engineering / Electronics / Optoelectronics},
  language = {en}
}

@article{singlaStringMatchingAlgorithms2012a,
  title = {String {{Matching Algorithms}} and Their {{Applicability}} in Various {{Applications}}},
  author = {Singla, Nimisha and Garg, Deepak},
  year = {2012},
  month = jan,
  volume = {1},
  abstract = {In this paper the applicability of the various strings matching algorithms are being described. Which algorithm is best in which application and why. This describes the optimal algorithm for various activities that include string matching as an important aspect of functionality. In all applications test string and pattern class needs to be matched always.},
  journal = {International Journal of Soft Computing and Engineering (IJSCE)}
}

@article{smithDisciplinedApproachNeural2018,
  title = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author = {Smith, Leslie N.},
  year = {2018},
  month = apr,
  abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.},
  archivePrefix = {arXiv},
  eprint = {1803.09820},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/XHLWQ3DD/Smith - 2018 - A disciplined approach to neural network hyper-par.pdf;/Users/antonio/Zotero/storage/98PT4CF8/1803.html},
  journal = {arXiv:1803.09820 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{stanevHighEnergyCosmic2010,
  title = {High {{Energy Cosmic Rays}}},
  author = {Stanev, Todor},
  year = {2010},
  month = mar,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Cosmic rays are an essential part of the universe. Their origin is related to many important astrophysical processes, such as star formation, stellar evolution, supernova explosions and the state of interstellar matter in the Galaxy. Cosmic Ray Physics reviews our present knowledge of cosmic rays, describing how they are born in a wide range of cosmic processes, how they are accelerated and how they interact with matter, magnetic fields and radiation during their journey across the Galaxy. The book also describes the detection of cosmic rays, and the processes which take place, both at the top and within the Earth's atmosphere. The author also describes the very important area of the underground detection of very high energy cosmic rays and particles such as neutrinos. The book is divided into two parts, the first describing the standard model of cosmic rays and contemporary challenges, and the second part dealing with very high energy cosmic rays that cannot be detected directly in satellite and balloon experiments, and with gamma-ray and neutrino astronomy. It is in this particular aspect of the book that the greatest developments have taken place during the 5 years since the first edition was completed. Consequently, it is in the chapters cosmic ray showers, their spectrum, on high energy neutrinos, and on gamma-ray astronomy of this revised and updated 2nd edition that a considerable amount of new material has been incorporated with more minor revisions and updating taking place in the first part of the book. Students and lecturers of advanced undergraduate courses on cosmic rays and astroparticle physics as well as post graduates and researchers will continue to find this book a valuable source of learning and reference.},
  googlebooks = {1y9YEYHAfBMC},
  isbn = {978-3-540-85148-6},
  keywords = {Science / Astronomy,Science / Physics / Astrophysics,Science / Physics / Nuclear,Science / Physics / Quantum Theory,Science / Physics / Relativity,Technology \& Engineering / Aeronautics \& Astronautics},
  language = {en}
}

@incollection{stanevOverview2010,
  title = {Overview},
  booktitle = {High {{Energy Cosmic Rays}}},
  author = {Stanev, Todor},
  year = {2010},
  month = mar,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Cosmic rays are an essential part of the universe. Their origin is related to many important astrophysical processes, such as star formation, stellar evolution, supernova explosions and the state of interstellar matter in the Galaxy. Cosmic Ray Physics reviews our present knowledge of cosmic rays, describing how they are born in a wide range of cosmic processes, how they are accelerated and how they interact with matter, magnetic fields and radiation during their journey across the Galaxy. The book also describes the detection of cosmic rays, and the processes which take place, both at the top and within the Earth's atmosphere. The author also describes the very important area of the underground detection of very high energy cosmic rays and particles such as neutrinos. The book is divided into two parts, the first describing the standard model of cosmic rays and contemporary challenges, and the second part dealing with very high energy cosmic rays that cannot be detected directly in satellite and balloon experiments, and with gamma-ray and neutrino astronomy. It is in this particular aspect of the book that the greatest developments have taken place during the 5 years since the first edition was completed. Consequently, it is in the chapters cosmic ray showers, their spectrum, on high energy neutrinos, and on gamma-ray astronomy of this revised and updated 2nd edition that a considerable amount of new material has been incorporated with more minor revisions and updating taking place in the first part of the book. Students and lecturers of advanced undergraduate courses on cosmic rays and astroparticle physics as well as post graduates and researchers will continue to find this book a valuable source of learning and reference.},
  googlebooks = {1y9YEYHAfBMC},
  isbn = {978-3-540-85148-6},
  keywords = {Science / Astronomy,Science / Physics / Astrophysics,Science / Physics / Nuclear,Science / Physics / Quantum Theory,Science / Physics / Relativity,Technology \& Engineering / Aeronautics \& Astronautics},
  language = {en}
}

@misc{stoddenSettingDefaultReproducible2013,
  title = {Setting the {{Default}} to {{Reproducible Reproducibility}} in {{Computational}} and {{Experimental Mathematics}}},
  author = {Stodden, Victoria and Bailey, David H. and Borwein, Jonathan M. and LeVeque, Randall J. and Rider, William J. and Stein, William},
  year = {2013},
  abstract = {Science is built upon foundations of theory and experiment validated and improved through open, transparent communication. With the increasingly central role of computation in scientific discovery this means communicating all details of the computations needed for others to replicate the experiment, i.e. making available to others the associated data and code. The ``reproducible research'' movement recognizes that traditional scientific research and publication practices now fall short of this ideal, and encourages all those involved in the production of computational science - scientists who use computational methods and the institutions that employ them, journals and dissemination mechanisms, and funding agencies - to facilitate and practice really reproducible research. This report summarizes discussions that took place during the ICERM Workshop on Reproducibility in Computational and Experimental Mathematics, held December 10-14, 2012. The main recommendations that emerged from the workshop discussions are: 1. It is important to promote a culture change that will integrate computational reproducibility into the research process.},
  file = {/Users/antonio/Zotero/storage/PW42ITTZ/992647adcc7e3626768841acb039d2b4a70d5c95.html},
  howpublished = {/paper/Setting-the-Default-to-Reproducible-Reproducibility-Stodden-Bailey/992647adcc7e3626768841acb039d2b4a70d5c95},
  journal = {undefined},
  language = {en}
}

@article{sublimeAutomaticPostDisasterDamage2019,
  title = {Automatic {{Post}}-{{Disaster Damage Mapping Using Deep}}-{{Learning Techniques}} for {{Change Detection}}: {{Case Study}} of the {{Tohoku Tsunami}}},
  shorttitle = {Automatic {{Post}}-{{Disaster Damage Mapping Using Deep}}-{{Learning Techniques}} for {{Change Detection}}},
  author = {Sublime, Jeremie and Kalinicheva, Ekaterina},
  year = {2019},
  month = may,
  volume = {11},
  pages = {1123},
  doi = {10.3390/rs11091123},
  abstract = {Post-disaster damage mapping is an essential task following tragic events such as hurricanes, earthquakes, and tsunamis. It is also a time-consuming and risky task that still often requires the sending of experts on the ground to meticulously map and assess the damages. Presently, the increasing number of remote-sensing satellites taking pictures of Earth on a regular basis with programs such as Sentinel, ASTER, or Landsat makes it easy to acquire almost in real time images from areas struck by a disaster before and after it hits. While the manual study of such images is also a tedious task, progress in artificial intelligence and in particular deep-learning techniques makes it possible to analyze such images to quickly detect areas that have been flooded or destroyed. From there, it is possible to evaluate both the extent and the severity of the damages. In this paper, we present a state-of-the-art deep-learning approach for change detection applied to satellite images taken before and after the Tohoku tsunami of 2011. We compare our approach with other machine-learning methods and show that our approach is superior to existing techniques due to its unsupervised nature, good performance, and relative speed of analysis.},
  file = {/Users/antonio/Zotero/storage/34GNHCCJ/Sublime and Kalinicheva - 2019 - Automatic Post-Disaster Damage Mapping Using Deep-.pdf},
  journal = {Remote Sensing}
}

@book{sutton1998introduction,
  title = {Introduction to Reinforcement Learning},
  author = {Sutton, Richard S and Barto, Andrew G and others},
  year = {1998},
  volume = {135},
  publisher = {{MIT press Cambridge}}
}

@article{thepierreaugercollaborationInferencesMassComposition2017,
  title = {Inferences on {{Mass Composition}} and {{Tests}} of {{Hadronic Interactions}} from 0.3 to 100 {{EeV}} Using the Water-{{Cherenkov Detectors}} of the {{Pierre Auger Observatory}}},
  author = {The Pierre Auger Collaboration and Aab, A. and Abreu, P. and Aglietta, M. and Samarai, I. Al and Albuquerque, I. F. M. and Allekotte, I. and Almela, A. and Castillo, J. Alvarez and {Alvarez-Mu{\~n}iz}, J. and Anastasi, G. A. and Anchordoqui, L. and Andrada, B. and Andringa, S. and Aramo, C. and Arqueros, F. and Arsene, N. and Asorey, H. and Assis, P. and Aublin, J. and Avila, G. and Badescu, A. M. and Balaceanu, A. and Barbato, F. and Luz, R. J. Barreira and Beatty, J. J. and Becker, K. H. and Bellido, J. A. and Berat, C. and Bertaina, M. E. and Bertou, X. and Biermann, P. L. and Biteau, J. and Blaess, S. G. and Blanco, A. and Blazek, J. and Bleve, C. and Boh{\'a}{\v c}ov{\'a}, M. and Boncioli, D. and Bonifazi, C. and Borodai, N. and Botti, A. M. and Brack, J. and Brancus, I. and Bretz, T. and Bridgeman, A. and Briechle, F. L. and Buchholz, P. and Bueno, A. and Buitink, S. and Buscemi, M. and {Caballero-Mora}, K. S. and Caccianiga, L. and Cancio, A. and Canfora, F. and Caramete, L. and Caruso, R. and Castellina, A. and Catalani, F. and Cataldi, G. and Cazon, L. and Chavez, A. G. and Chinellato, J. A. and Chudoba, J. and Clay, R. W. and Cobos, A. and Colalillo, R. and Coleman, A. and Collica, L. and Coluccia, M. R. and Concei{\c c}{\~a}o, R. and Consolati, G. and Contreras, F. and Cooper, M. J. and Coutu, S. and Covault, C. E. and Cronin, J. and D'Amico, S. and Daniel, B. and Dasso, S. and Daumiller, K. and Dawson, B. R. and {de Almeida}, R. M. and {de Jong}, S. J. and De Mauro, G. and Neto, J. R. T. de Mello and De Mitri, I. and {de Oliveira}, J. and {de Souza}, V. and Debatin, J. and Deligny, O. and Castro, M. L. D{\'i}az and Diogo, F. and Dobrigkeit, C. and D'Olivo, J. C. and Dorosti, Q. and dos Anjos, R. C. and Dova, M. T. and Dundovic, A. and Ebr, J. and Engel, R. and Erdmann, M. and Erfani, M. and Escobar, C. O. and Espadanal, J. and Etchegoyen, A. and Falcke, H. and Farmer, J. and Farrar, G. and Fauth, A. C. and Fazzini, N. and Fenu, F. and Fick, B. and Figueira, J. M. and Filip{\v c}i{\v c}, A. and Fratu, O. and Freire, M. M. and Fujii, T. and Fuster, A. and Gaior, R. and Garc{\'i}a, B. and {Garcia-Pinto}, D. and Gat{\'e}, F. and Gemmeke, H. and {Gherghel-Lascu}, A. and Ghia, P. L. and Giaccari, U. and Giammarchi, M. and Giller, M. and G{\l}as, D. and Glaser, C. and Golup, G. and Berisso, M. G{\'o}mez and Vitale, P. F. G{\'o}mez and Gonz{\'a}lez, N. and Gorgi, A. and Gorham, P. and Grillo, A. F. and Grubb, T. D. and Guarino, F. and Guedes, G. P. and Halliday, R. and Hampel, M. R. and Hansen, P. and Harari, D. and Harrison, T. A. and Harton, J. L. and Haungs, A. and Hebbeker, T. and Heck, D. and Heimann, P. and Herve, A. E. and Hill, G. C. and Hojvat, C. and Holt, E. and Homola, P. and H{\"o}randel, J. R. and Horvath, P. and Hrabovsk{\'y}, M. and Huege, T. and Hulsman, J. and Insolia, A. and Isar, P. G. and Jandt, I. and Johnsen, J. A. and Josebachuili, M. and Jurysek, J. and K{\"a}{\"a}p{\"a}, A. and Kambeitz, O. and Kampert, K. H. and Keilhauer, B. and Kemmerich, N. and Kemp, E. and Kemp, J. and Kieckhafer, R. M. and Klages, H. O. and Kleifges, M. and Kleinfeller, J. and Krause, R. and Krohm, N. and Kuempel, D. and Mezek, G. Kukec and Kunka, N. and Awad, A. Kuotb and Lago, B. L. and LaHurd, D. and Lang, R. G. and Lauscher, M. and Legumina, R. and {de Oliveira}, M. A. Leigui and {Letessier-Selvon}, A. and {Lhenry-Yvon}, I. and Link, K. and Presti, D. Lo and Lopes, L. and L{\'o}pez, R. and Casado, A. L{\'o}pez and Lorek, R. and Luce, Q. and Lucero, A. and Malacari, M. and Mallamaci, M. and Mandat, D. and Mantsch, P. and Mariazzi, A. G. and Mari{\c s}, I. C. and Marsella, G. and Martello, D. and Martinez, H. and Bravo, O. Mart{\'i}nez and Meza, J. J. Mas{\'i}as and Mathes, H. J. and Mathys, S. and Matthews, J. and Matthews, J. A. J. and Matthiae, G. and Mayotte, E. and Mazur, P. O. and Medina, C. and {Medina-Tanco}, G. and Melo, D. and Menshikov, A. and Merenda, K.-D. and Michal, S. and Micheletti, M. I. and Middendorf, L. and Miramonti, L. and Mitrica, B. and Mockler, D. and Mollerach, S. and Montanet, F. and Morello, C. and Mostaf{\'a}, M. and M{\"u}ller, A. L. and M{\"u}ller, G. and Muller, M. A. and M{\"u}ller, S. and Mussa, R. and Naranjo, I. and Nellen, L. and Nguyen, P. H. and {Niculescu-Oglinzanu}, M. and Niechciol, M. and Niemietz, L. and Niggemann, T. and Nitz, D. and Nosek, D. and Novotny, V. and No{\v z}ka, L. and N{\'u}{\~n}ez, L. A. and Ochilo, L. and Oikonomou, F. and Olinto, A. and Palatka, M. and Pallotta, J. and Papenbreer, P. and Parente, G. and Parra, A. and Paul, T. and Pech, M. and Pedreira, F. and P{\k{e}}kala, J. and Pelayo, R. and {Pe{\~n}a-Rodriguez}, J. and Pereira, L. A. S. and Perlin, M. and Perrone, L. and Peters, C. and Petrera, S. and Phuntsok, J. and Piegaia, R. and Pierog, T. and Pimenta, M. and Pirronello, V. and Platino, M. and Plum, M. and Porowski, C. and Prado, R. R. and Privitera, P. and Prouza, M. and Quel, E. J. and Querchfeld, S. and Quinn, S. and {Ramos-Pollan}, R. and Rautenberg, J. and Ravignani, D. and Ridky, J. and Riehn, F. and Risse, M. and Ristori, P. and Rizi, V. and {de Carvalho}, W. Rodrigues and Fernandez, G. Rodriguez and Rojo, J. Rodriguez and Rogozin, D. and Roncoroni, M. J. and Roth, M. and Roulet, E. and Rovero, A. C. and Ruehl, P. and Saffi, S. J. and Saftoiu, A. and Salamida, F. and Salazar, H. and Saleh, A. and Greus, F. Salesa and Salina, G. and S{\'a}nchez, F. and {Sanchez-Lucas}, P. and Santos, E. M. and Santos, E. and Sarazin, F. and Sarmento, R. and {Sarmiento-Cano}, C. and Sato, R. and Schauer, M. and Scherini, V. and Schieler, H. and Schimp, M. and Schmidt, D. and Scholten, O. and Schov{\'a}nek, P. and Schr{\"o}der, F. G. and Schr{\"o}der, S. and Schulz, A. and Schumacher, J. and Sciutto, S. J. and Segreto, A. and Shadkam, A. and Shellard, R. C. and Sigl, G. and Silli, G. and Sima, O. and {\'S}mia{\l}kowski, A. and {\v S}m{\'i}da, R. and Snow, G. R. and Sommers, P. and Sonntag, S. and Squartini, R. and Stanca, D. and Stani{\v c}, S. and Stasielak, J. and Stassi, P. and Stolpovskiy, M. and Strafella, F. and Streich, A. and Suarez, F. and Dur{\'a}n, M. Suarez and Sudholz, T. and Suomij{\"a}rvi, T. and Supanitsky, A. D. and {\v S}up{\'i}k, J. and Swain, J. and Szadkowski, Z. and Taboada, A. and Taborda, O. A. and Theodoro, V. M. and Timmermans, C. and Peixoto, C. J. Todero and Tomankova, L. and Tom{\'e}, B. and Elipe, G. Torralba and Travnicek, P. and Trini, M. and Ulrich, R. and Unger, M. and Urban, M. and Galicia, J. F. Vald{\'e}s and Vali{\~n}o, I. and Valore, L. and {van Aar}, G. and {van Bodegom}, P. and van den Berg, A. M. and {van Vliet}, A. and Varela, E. and C{\'a}rdenas, B. Vargas and Varner, G. and V{\'a}zquez, R. A. and Veberi{\v c}, D. and Ventura, C. and Quispe, I. D. Vergara and Verzi, V. and Vicha, J. and Villase{\~n}or, L. and Vorobiov, S. and Wahlberg, H. and Wainberg, O. and Walz, D. and Watson, A. A. and Weber, M. and Weindl, A. and Wiencke, L. and Wilczy{\'n}ski, H. and Wirtz, M. and Wittkowski, D. and Wundheiler, B. and Yang, L. and Yushkov, A. and Zas, E. and Zavrtanik, D. and Zavrtanik, M. and Zepeda, A. and Zimmermann, B. and Ziolkowski, M. and Zong, Z. and Zuccarello, F.},
  year = {2017},
  month = oct,
  abstract = {We present a new method for probing the hadronic interaction models at ultra-high energy and extracting details about mass composition. This is done using the time profiles of the signals recorded with the water-Cherenkov detectors of the Pierre Auger Observatory. The profiles arise from a mix of the muon and electromagnetic components of air-showers. Using the risetimes of the recorded signals we define a new parameter, which we use to compare our observations with predictions from simulations. We find, firstly, inconsistencies between our data and predictions over a greater energy range and with substantially more events than in previous studies. Secondly, by calibrating the new parameter with fluorescence measurements from observations made at the Auger Observatory, we can infer the depth of shower maximum for a sample of over 81,000 events extending from 0.3 EeV to over 100 EeV. Above 30 EeV, the sample is nearly fourteen times larger than currently available from fluorescence measurements and extending the covered energy range by half a decade. The energy dependence of the average depth of shower maximum is compared to simulations and interpreted in terms of the mean of the logarithmic mass. We find good agreement with previous work and extend the measurement of the mean depth of shower maximum to greater energies than before, reducing significantly the statistical uncertainty associated with the inferences about mass composition.},
  archivePrefix = {arXiv},
  eprint = {1710.07249},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/VA3KUFXK/The Pierre Auger Collaboration et al. - 2017 - Inferences on Mass Composition and Tests of Hadron.pdf;/Users/antonio/Zotero/storage/U686M267/1710.html},
  journal = {arXiv:1710.07249 [astro-ph]},
  keywords = {Astrophysics - High Energy Astrophysical Phenomena},
  primaryClass = {astro-ph}
}

@article{todoli-signesAlgorithmsArtificialIntelligence2019,
  title = {Algorithms, Artificial Intelligence and Automated Decisions Concerning Workers and the Risks of Discrimination: The Necessary Collective Governance of Data Protection},
  shorttitle = {Algorithms, Artificial Intelligence and Automated Decisions Concerning Workers and the Risks of Discrimination},
  author = {{Todol{\'i}-Signes}, Adri{\'a}n},
  year = {2019},
  month = nov,
  volume = {25},
  pages = {465--481},
  publisher = {{SAGE Publications Ltd}},
  issn = {1024-2589},
  doi = {10.1177/1024258919876416},
  abstract = {Big data, algorithms and artificial intelligence now allow employers to process information on their employees and potential employees in a far more efficient manner and at a much lower cost than in the past. This makes it possible to profile workers automatically and even allows technology itself to replace human resources personnel in making decisions that have legal effects on employees (recruitment, promotion, dismissals, etc.). This entails great risks of worker discrimination and defencelessness, with workers unaware of the reasons underlying any such decision. This article analyses the protections established in the EU General Data Protection Regulation (GDPR) for safeguarding employees against discrimination. One of the main conclusions that can be drawn is that, in the face of the inadequacy of the GDPR in the field of labour relations, there is a need for the collective governance of workplace data protection, requiring the participation of workers' representatives in establishing safeguards.},
  file = {/Users/antonio/Zotero/storage/7YVQKJXS/Todolí-Signes - 2019 - Algorithms, artificial intelligence and automated .pdf},
  journal = {Transfer: European Review of Labour and Research},
  language = {en},
  number = {4}
}

@misc{Trello,
  title = {Trello},
  abstract = {Infinitamente flexible. Incre\'iblemente f\'acil de utilizar. Aplicaciones m\'oviles excelentes. Es gratis. Trello registra todo, desde el proyecto general hasta el m\'as minucioso detalle.},
  file = {/Users/antonio/Zotero/storage/BL7QVXXZ/trello.com.html},
  howpublished = {https://trello.com}
}

@book{trippiArtificialIntelligenceFinance1995,
  title = {Artificial {{Intelligence}} in {{Finance}} and {{Investing}}: {{State}}-of-the-{{Art Technologies}} for {{Securities Selection}} and {{Portfolio Management}}},
  shorttitle = {Artificial {{Intelligence}} in {{Finance}} and {{Investing}}},
  author = {Trippi, Robert R. and Lee, Jae K.},
  year = {1995},
  edition = {1st},
  publisher = {{McGraw-Hill, Inc.}},
  address = {{USA}},
  abstract = {From the Publisher: In Artificial Intelligence in Finance and Investing, authors Robert Trippi and Jae Lee explain this fascinating new technology in terms that portfolio managers, institutional investors, investment analysis, and information systems professionals can understand. Using real-life examples and a practical approach, this rare and readable volume discusses the entire field of artificial intelligence of relevance to investing, so that readers can realize the benefits and evaluate the features of existing or proposed systems, and ultimately construct their own systems. Topics include using Expert Systems for Asset Allocation, Timing Decisions, Pattern Recognition, and Risk Assessment; overview of Popular Knowledge-Based Systems; construction of Synergistic Rule Bases for Securities Selection; incorporating the Markowitz Portfolio Optimization Model into Knowledge-Based Systems; Bayesian Theory and Fuzzy Logic System Components; Machine Learning in Portfolio Selection and Investment Timing, including Pattern-Based Learning and Fenetic Algorithms; and Neural Network-Based Systems. To illustrate the concepts presented in the book, the authors conclude with a valuable practice session and analysis of a typical knowledge-based system for investment management, K-FOLIO. For those who want to stay on the cutting edge of the "application" revolution, Artificial Intelligence in Finance and Investing offers a pragmatic introduction to the use of knowledge-based systems in securities selection and portfolio management.},
  isbn = {978-1-55738-868-1}
}

@misc{UCIMachineLearning,
  title = {{{UCI Machine Learning Repository}}: {{Adult Data Set}}},
  file = {/Users/antonio/Zotero/storage/LV8AEJJ8/Adult.html},
  howpublished = {http://archive.ics.uci.edu/ml/datasets/Adult}
}

@article{vincentConnectionScoreMatching2011,
  title = {A {{Connection Between Score Matching}} and {{Denoising Autoencoders}}},
  author = {Vincent, Pascal},
  year = {2011},
  month = apr,
  volume = {23},
  pages = {1661--1674},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00142},
  abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
  file = {/Users/antonio/Zotero/storage/JC2NQ326/Vincent - 2011 - A Connection Between Score Matching and Denoising .pdf;/Users/antonio/Zotero/storage/NLWYBUYM/NECO_a_00142.html},
  journal = {Neural Computation},
  number = {7}
}

@article{wangAutoencoderBasedDimensionality2016,
  title = {Auto-Encoder Based Dimensionality Reduction},
  author = {Wang, Yasi and Yao, Hongxun and Zhao, Sicheng},
  year = {2016},
  month = apr,
  volume = {184},
  pages = {232--242},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2015.08.104},
  abstract = {Auto-encoder\textemdash a tricky three-layered neural network, known as auto-association before, constructs the ``building block'' of deep learning, which has been demonstrated to achieve good performance in various domains. In this paper, we try to investigate the dimensionality reduction ability of auto-encoder, and see if it has some kind of good property that might accumulate when being stacked and thus contribute to the success of deep learning. Based on the above idea, this paper starts from auto-encoder and focuses on its ability to reduce the dimensionality, trying to understand the difference between auto-encoder and state-of-the-art dimensionality reduction methods. Experiments are conducted both on the synthesized data for an intuitive understanding of the method, mainly on two and three-dimensional spaces for better visualization, and on some real datasets, including MNIST and Olivetti face datasets. The results show that auto-encoder can indeed learn something different from other methods. Besides, we preliminarily investigate the influence of the number of hidden layer nodes on the performance of auto-encoder and its possible relation with the intrinsic dimensionality of input data.},
  file = {/Users/antonio/Zotero/storage/MC8INZTR/S0925231215017671.html},
  journal = {Neurocomputing},
  keywords = {Auto-encoder,Dimensionality reduction,Dimensionality-accuracy,Intrinsic dimensionality,Visualization},
  language = {en},
  series = {{{RoLoD}}: {{Robust Local Descriptors}} for {{Computer Vision}} 2014}
}

@book{wardenBigDataGlossary2011,
  title = {Big {{Data Glossary}}},
  author = {Warden, Pete},
  year = {2011},
  publisher = {{O'Reilly Media, Inc.}},
  abstract = {To help you navigate the large number of new data tools available, this guide describes 60 of the most recent innovations, from NoSQL databases and MapReduce approaches to machine learning and visualization tools. Descriptions are based on first-hand experience with these tools in a production environment. This handy glossary also includes a chapter of key terms that help define many of these tool categories:NoSQL DatabasesDocument-oriented databases using a key/value interface rather than SQL MapReduceTools that support distributed computing on large datasets StorageTechnologies for storing data in a distributed way ServersWays to rent computing power on remote machines ProcessingTools for extracting valuable information from large datasets Natural Language ProcessingMethods for extracting information from human-created text Machine LearningTools that automatically perform data analyses, based on results of a one-off analysis VisualizationApplications that present meaningful data graphically AcquisitionTechniques for cleaning up messy public data sources SerializationMethods to convert data structure or object state into a storable format},
  isbn = {978-1-4493-1459-0}
}

@article{wen2016cat2vec,
  title = {{{Cat2Vec}}: {{Learning}} Distributed Representation of Multi-Field Categorical Data},
  author = {Wen, Ying and Wang, Jun and Chen, Tianyao and Zhang, Weinan},
  year = {2016}
}

@inproceedings{wengMonocular3DObject2019,
  title = {Monocular {{3D Object Detection}} with {{Pseudo}}-{{LiDAR Point Cloud}}},
  author = {Weng, Xinshuo and Kitani, Kris},
  year = {2019},
  month = aug,
  doi = {10.1109/ICCVW.2019.00114},
  abstract = {Monocular 3D scene understanding tasks, such as object size estimation, heading angle estimation and 3D localization, is challenging. Successful modern day methods for 3D scene understanding require the use of a 3D sensor. On the other hand, single image based methods have significantly worse performance. In this work, we aim at bridging the performance gap between 3D sensing and 2D sensing for 3D object detection by enhancing LiDAR-based algorithms to work with single image input. Specifically, we perform monocular depth estimation and lift the input image to a point cloud representation, which we call pseudo-LiDAR point cloud. Then we can train a LiDAR-based 3D detection network with our pseudo-LiDAR end-to-end. Following the pipeline of two-stage 3D detection algorithms, we detect 2D object proposals in the input image and extract a point cloud frustum from the pseudo-LiDAR for each proposal. Then an oriented 3D bounding box is detected for each frustum. To handle the large amount of noise in the pseudo-LiDAR, we propose two innovations: (1) use a 2D-3D bounding box consistency constraint, adjusting the predicted 3D bounding box to have a high overlap with its corresponding 2D proposal after projecting onto the image ; (2) use the instance mask instead of the bounding box as the representation of 2D proposals, in order to reduce the number of points not belonging to the object in the point cloud frustum. Through our evaluation on the KITTI benchmark, we achieve the top-ranked performance on both bird's eye view and 3D object detection among all monocular methods, effectively quadrupling the performance over previous state-of-the-art. Our code is available at https: //github.com/xinshuoweng/Mono3D\_PLiDAR.},
  file = {/Users/antonio/Zotero/storage/L7UE25PV/Weng and Kitani - 2019 - Monocular 3D Object Detection with Pseudo-LiDAR Po.pdf}
}

@incollection{williamsUsingNystromMethod2001,
  title = {Using the {{Nystr\"om Method}} to {{Speed Up Kernel Machines}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 13},
  author = {Williams, Christopher K. I. and Seeger, Matthias},
  editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
  year = {2001},
  pages = {682--688},
  publisher = {{MIT Press}},
  file = {/Users/antonio/Zotero/storage/CE9MEXF9/Williams and Seeger - 2001 - Using the Nyström Method to Speed Up Kernel Machin.pdf;/Users/antonio/Zotero/storage/7JHN8ST3/1866-using-the-nystrom-method-to-speed-up-kernel-machines.html}
}

@article{wilsonGoodEnoughPractices2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2017},
  month = jun,
  volume = {13},
  pages = {e1005510},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005510},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  file = {/Users/antonio/Zotero/storage/3M2ALCSN/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf;/Users/antonio/Zotero/storage/WIAH2LBI/article.html},
  journal = {PLOS Computational Biology},
  keywords = {Computer software,Control systems,Data management,Data processing,Programming languages,Reproducibility,Software tools,Source code},
  language = {en},
  number = {6}
}

@incollection{wrigleyTamingArtificialIntelligence2018,
  title = {Taming {{Artificial Intelligence}}: ``{{Bots}},'' the {{GDPR}} and {{Regulatory Approaches}}},
  shorttitle = {Taming {{Artificial Intelligence}}},
  booktitle = {Robotics, {{AI}} and the {{Future}} of {{Law}}},
  author = {Wrigley, Sam},
  editor = {Corrales, Marcelo and Fenwick, Mark and Forg{\'o}, Nikolaus},
  year = {2018},
  pages = {183--208},
  publisher = {{Springer}},
  address = {{Singapore}},
  doi = {10.1007/978-981-13-2874-9_8},
  abstract = {Bots and AI have the potential to revolutionize the way that personal data is processed. Unlike processing performed by traditional methods, they have an unprecedented ability (and patience) to gather, analyze and combine information. However, the introduction of ``smarter'' computers does not always mean that the nature of the processing will change; often, the result will be substantially similar to processing by a human. We cannot, then, regulate processing by bots and AI as a sui generis concept. This chapter examines the different regulatory approaches that exist under the new General Data Protection Regulation (the GDPR)\textemdash the general regulatory approach (which treats all processing in the same way), the specific regulatory approach (which imposes specific rules for automated processing) and the co-regulatory approach (where data controllers are required to analyze and mitigate the risks on their own). It then considers how these approaches interact and makes some recommendations for how they should be interpreted and implemented in the future.},
  isbn = {9789811328749},
  keywords = {AI,Bots,Data protection,GDPR,Regulatory methods},
  language = {en},
  series = {Perspectives in {{Law}}, {{Business}} and {{Innovation}}}
}

@incollection{wrigleyTamingArtificialIntelligence2018a,
  title = {Taming {{Artificial Intelligence}}: ``{{Bots}},'' the {{GDPR}} and {{Regulatory Approaches}}},
  shorttitle = {Taming {{Artificial Intelligence}}},
  booktitle = {Robotics, {{AI}} and the {{Future}} of {{Law}}},
  author = {Wrigley, Sam},
  editor = {Corrales, Marcelo and Fenwick, Mark and Forg{\'o}, Nikolaus},
  year = {2018},
  pages = {183--208},
  publisher = {{Springer}},
  address = {{Singapore}},
  doi = {10.1007/978-981-13-2874-9_8},
  abstract = {Bots and AI have the potential to revolutionize the way that personal data is processed. Unlike processing performed by traditional methods, they have an unprecedented ability (and patience) to gather, analyze and combine information. However, the introduction of ``smarter'' computers does not always mean that the nature of the processing will change; often, the result will be substantially similar to processing by a human. We cannot, then, regulate processing by bots and AI as a sui generis concept. This chapter examines the different regulatory approaches that exist under the new General Data Protection Regulation (the GDPR)\textemdash the general regulatory approach (which treats all processing in the same way), the specific regulatory approach (which imposes specific rules for automated processing) and the co-regulatory approach (where data controllers are required to analyze and mitigate the risks on their own). It then considers how these approaches interact and makes some recommendations for how they should be interpreted and implemented in the future.},
  file = {/Users/antonio/Zotero/storage/UNBHQJBL/Wrigley - 2018 - Taming Artificial Intelligence “Bots,” the GDPR a.pdf},
  isbn = {9789811328749},
  keywords = {AI,Bots,Data protection,GDPR,Regulatory methods},
  language = {en},
  series = {Perspectives in {{Law}}, {{Business}} and {{Innovation}}}
}

@article{wuFeatureRescalingSupport2011,
  title = {Feature Rescaling of Support Vector Machines},
  author = {Wu, Zhengpeng and Zhang, Xuegong},
  year = {2011},
  month = aug,
  volume = {16},
  pages = {414--421},
  issn = {1007-0214},
  doi = {10.1016/S1007-0214(11)70060-8},
  abstract = {Support vector machines (SVMs) have widespread use in various classification problems. Although SVMs are often used as an off-the-shelf tool, there are still some important issues which require improvement, such as feature rescaling. Standardization is the most commonly used feature rescaling method. However, standardization does not always improve classification accuracy. This paper describes two feature rescaling methods: multiple kernel learning-based rescaling (MKL-SVM) and kernel-target alignment-based rescaling (KTA-SVM). MKL-SVM makes use of the framework of multiple kernel learning (MKL) and KTA-SVM is built upon the concept of kernel alignment, which measures the similarity between kernels. The proposed methods were compared with three other methods: an SVM method without rescaling, an SVM method with standardization, and SCADSVM. Test results demonstrate that different rescaling methods apply to different situations and that the proposed methods outperform the others in general.},
  file = {/Users/antonio/Zotero/storage/D2PNS44X/6078023.html},
  journal = {Tsinghua Science and Technology},
  keywords = {feature rescaling,Kernel,kernel-target alignment (KTA),Matrix decomposition,multiple kernel learning (MKL),Noise measurement,Standardization,Support vector machines,support vector machines (SVMs),Training,Vectors},
  number = {4}
}

@techreport{yavuzMachineBiasArtificial2019,
  title = {Machine {{Bias}}: {{Artificial Intelligence}} and {{Discrimination}}},
  shorttitle = {Machine {{Bias}}},
  author = {Yavuz, Can},
  year = {2019},
  month = aug,
  pages = {44--64},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3439702},
  abstract = {The past decade has seen the rapid development of artificial intelligence. It has resulted in extensive usage and reliance within many diverse fields that influences our daily lives as well as human rights, and especially the prohibition of discrimination. The thesis examines artificial intelligence discrimination and asks why and how it occurs, who is (more likely to be) affected by it, and how policymakers should respond to protect human rights. The findings reveal that artificial intelligence discriminates in various ways, and the most vulnerable and discriminated groups are more likely to be victims of it. Many problems in the field stem from lack of regulation and over-reliance on artificial intelligence. This thesis makes a preliminary recommendation and invites policymakers to cautiously regulate artificial intelligence to prevent artificial intelligence discrimination.},
  file = {/Users/antonio/Zotero/storage/LTARAK4T/Yavuz - 2019 - Machine Bias Artificial Intelligence and Discrimi.pdf;/Users/antonio/Zotero/storage/XKGGSPRH/papers.html},
  keywords = {artificial intelligence,discrimination,human rights,prohibition of discrimination,regulation},
  language = {en},
  number = {ID 3439702},
  type = {{{SSRN Scholarly Paper}}}
}

@incollection{yavuzMachineBiasArtificial2019a,
  title = {Machine {{Bias}}: {{Artificial Intelligence}} and {{Discrimination}}},
  shorttitle = {Machine {{Bias}}},
  author = {Yavuz, Can},
  year = {2019},
  month = aug,
  publisher = {{Social Science Research Network}},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.3439702},
  abstract = {The past decade has seen the rapid development of artificial intelligence. It has resulted in extensive usage and reliance within many diverse fields that influences our daily lives as well as human rights, and especially the prohibition of discrimination. The thesis examines artificial intelligence discrimination and asks why and how it occurs, who is (more likely to be) affected by it, and how policymakers should respond to protect human rights. The findings reveal that artificial intelligence discriminates in various ways, and the most vulnerable and discriminated groups are more likely to be victims of it. Many problems in the field stem from lack of regulation and over-reliance on artificial intelligence. This thesis makes a preliminary recommendation and invites policymakers to cautiously regulate artificial intelligence to prevent artificial intelligence discrimination.},
  keywords = {artificial intelligence,discrimination,human rights,prohibition of discrimination,regulation},
  language = {en}
}

@article{zahariaAcceleratingMachineLearning2018,
  title = {Accelerating the {{Machine Learning Lifecycle}} with {{MLflow}}},
  author = {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
  year = {2018},
  abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLflow, an open source platform we recently launched to streamline the machine learning lifecycle. MLflow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
  journal = {IEEE Data Eng. Bull.}
}

@article{zhangVELCNewVariational2020,
  title = {{{VELC}}: {{A New Variational AutoEncoder Based Model}} for {{Time Series Anomaly Detection}}},
  shorttitle = {{{VELC}}},
  author = {Zhang, Chunkai and Li, Shaocong and Zhang, Hongye and Chen, Yingyang},
  year = {2020},
  month = apr,
  abstract = {Anomaly detection is a classical but worthwhile problem, and many deep learning-based anomaly detection algorithms have been proposed, which can usually achieve better detection results than traditional methods. In view of reconstruct ability of the model and the calculation of anomaly score, this paper proposes a time series anomaly detection method based on Variational AutoEncoder model(VAE) with re-Encoder and Latent Constraint network(VELC). In order to modify reconstruct ability of the model to prevent it from reconstructing abnormal samples well, we add a constraint network in the latent space of the VAE to force it generate new latent variables that are similar with that of training samples. To be able to calculate anomaly score in two feature spaces, we train a re-encoder to transform the generated data to a new latent space. For better handling the time series, we use the LSTM as the encoder and decoder part of the VAE framework. Experimental results of several benchmarks show that our method outperforms state-of-the-art anomaly detection methods.},
  archivePrefix = {arXiv},
  eprint = {1907.01702},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/X72S2LQU/Zhang et al. - 2020 - VELC A New Variational AutoEncoder Based Model fo.pdf;/Users/antonio/Zotero/storage/D9NI9ZFL/1907.html},
  journal = {arXiv:1907.01702 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{zhaoReproducingScientificExperiment2019,
  title = {Reproducing {{Scientific Experiment}} with {{Cloud DevOps}}},
  author = {Zhao, Feng and Niu, Xingzhi and Huang, Shao-Lun and Zhang, Lin},
  year = {2019},
  month = oct,
  abstract = {The reproducibility of scientific experiment is vital for the advancement of disciplines based on previous work. To achieve this goal, many researchers focus on complex methodology and self-invented tools which have difficulty in practical usage. In this article, we introduce the DevOps infrastructure from software engineering community and shows how DevOps can be used effectively to reproduce experiments for computer science related disciplines. DevOps can be enabled using freely available cloud computing machines for medium sized experiment and self-hosted computing engines for large scale computing, thus powering researchers to share their experiment result with others in a more reliable way.},
  archivePrefix = {arXiv},
  eprint = {1910.13397},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/TFRTFPCY/Zhao et al. - 2019 - Reproducing Scientific Experiment with Cloud DevOp.pdf;/Users/antonio/Zotero/storage/RB8SHB5D/1910.html},
  journal = {arXiv:1910.13397 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  primaryClass = {cs}
}

@article{zhuClassificationSupervisedAutoEncoder2020,
  title = {A {{Classification Supervised Auto}}-{{Encoder Based}} on {{Predefined Evenly}}-{{Distributed Class Centroids}}},
  author = {Zhu, Qiuyu and Zhang, Ruixin},
  year = {2020},
  month = jan,
  abstract = {Classic variational autoencoders are used to learn complex data distributions, that are built on standard function approximators. Especially, VAE has shown promise on a lot of complex task. In this paper, a new autoencoder model - classification supervised autoencoder (CSAE) based on predefined evenly-distributed class centroids (PEDCC) is proposed. Our method uses PEDCC of latent variables to train the network to ensure the maximization of inter-class distance and the minimization of inner-class distance. Instead of learning mean/variance of latent variables distribution and taking reparameterization of VAE, latent variables of CSAE are directly used to classify and as input of decoder. In addition, a new loss function is proposed to combine the loss function of classification. Based on the basic structure of the universal autoencoder, we realized the comprehensive optimal results of encoding, decoding, classification, and good model generalization performance at the same time. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1902.00220},
  eprinttype = {arxiv},
  file = {/Users/antonio/Zotero/storage/LRY729U7/Zhu and Zhang - 2020 - A Classification Supervised Auto-Encoder Based on .pdf;/Users/antonio/Zotero/storage/Z2CM3RNF/1902.html},
  journal = {arXiv:1902.00220 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}


